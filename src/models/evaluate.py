#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""evaluate.py Â· Dialog Summarization ê²½ì§„ëŒ€íšŒ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸"""

import os
import sys
import argparse
import pandas as pd
import yaml
from pathlib import Path
from typing import List, Dict, Optional

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, '..', '..')
sys.path.append(project_root)

# âœ… ì¤‘ì•™í™”ëœ metric ìœ í‹¸ë¦¬í‹° ë° í›„ì²˜ë¦¬ í•¨ìˆ˜ import
from src.utils.metrics import calculate_rouge_scores
from src.utils.postprocess import postprocess

def find_best_model_path(models_dir: str = "outputs/models") -> Optional[str]:
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ì°¾ê¸°"""
    best_link = os.path.join(models_dir, "best")
    if os.path.exists(best_link):
        return best_link
    
    # best ë§í¬ê°€ ì—†ìœ¼ë©´ latest ì‹œë„
    latest_link = os.path.join(models_dir, "latest")
    if os.path.exists(latest_link):
        print("âš ï¸ 'best' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ 'latest' ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return latest_link
    
    return None


def generate_predictions(model_path: str, ref_file: str, output_file: str) -> str:
    """ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±"""
    print(f"ğŸš€ ëª¨ë¸ ë¡œë“œ: {model_path}")
    
    try:
        from src.models.infer import inference as _infer
        
        # config ë¡œë“œ ë° ì„¤ì •
        project_root = Path(__file__).resolve().parent.parent.parent
        config_path = project_root / "src" / "config" / "config.yaml"
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)

        config["test_file"] = ref_file

        # test_pred íŒŒì¼ëª…ì— ëª¨ë¸ ì •ë³´ ì¶”ê°€
        if os.path.basename(output_file).startswith("test_pred"):
            model_dir_name = os.path.basename(os.path.realpath(model_path)) if model_path else "model"
            name, ext = os.path.splitext(output_file)
            output_file = f"{name}_{model_dir_name}{ext}"
        config["output_file"] = output_file

        print("ğŸ“ ì˜ˆì¸¡ ìƒì„± ì¤‘...")
        _infer(config, model_path=model_path)
        print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {output_file}")
        
        return output_file
    
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ìƒì„± ì‹¤íŒ¨: {e}")
        raise


def evaluate_predictions(pred_csv: str, ref_csv: str) -> Dict[str, float]:
    """ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€"""
    print(f"ğŸ“Š í‰ê°€ ì‹œì‘...")
    
    # íŒŒì¼ ê²€ì¦
    if not os.path.exists(pred_csv):
        raise FileNotFoundError(f"ì˜ˆì¸¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pred_csv}")
    if not os.path.exists(ref_csv):
        raise FileNotFoundError(f"ì •ë‹µ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ref_csv}")
    
    # ë°ì´í„° ë¡œë“œ
    pred_df = pd.read_csv(pred_csv)
    ref_df = pd.read_csv(ref_csv)
    
    # ì»¬ëŸ¼ ê²€ì¦
    if not {"fname", "summary"}.issubset(pred_df.columns):
        raise ValueError("ì˜ˆì¸¡ íŒŒì¼ì— 'fname', 'summary' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    if not {"fname", "summary"}.issubset(ref_df.columns):
        raise ValueError("ì •ë‹µ íŒŒì¼ì— 'fname', 'summary' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ë°ì´í„° ë³‘í•©
    merged = pd.merge(
        ref_df[["fname", "summary"]],
        pred_df[["fname", "summary"]].rename(columns={"summary": "pred"}),
        on="fname", 
        how="inner"
    )
    
    if len(merged) == 0:
        raise ValueError("ì˜ˆì¸¡ê³¼ ì •ë‹µ íŒŒì¼ì˜ fnameì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ“ˆ í‰ê°€ ëŒ€ìƒ: {len(merged)}ê°œ ìƒ˜í”Œ")

    # ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ì¶”ì¶œ
    preds = merged["pred"].tolist()
    refs_list = [str(s).split('|||' ) for s in merged["summary"]]

    # í›„ì²˜ë¦¬: train.py ì™€ ë™ì¼í•œ ê·œì¹™ ì ìš©
    project_root = Path(__file__).resolve().parent.parent.parent
    cfg_path = project_root / "src" / "config" / "config.yaml"
    with open(cfg_path, "r") as f:
        _cfg = yaml.safe_load(f)
    remove_tokens = _cfg.get('inference', {}).get('remove_tokens', None)
    preds = [postprocess(p, remove_tokens) for p in preds]

    # ROUGE ì ìˆ˜ ê³„ì‚°
    scores = calculate_rouge_scores(preds, refs_list)
    
    return scores


def print_results(scores: Dict[str, float], model_info: str = ""):
    """ê²°ê³¼ ì¶œë ¥"""
    print(f"\n{'='*50}")
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼ {model_info}")
    print(f"{'='*50}")
    print(f"ROUGE-1: {scores['rouge_1']:.4f}")
    print(f"ROUGE-2: {scores['rouge_2']:.4f}")
    print(f"ROUGE-L: {scores['rouge_l']:.4f}")
    print(f"{'='*50}")
    print(f"ğŸ¯ Final Score: {scores['final_score']:.4f}")
    print(f"{'='*50}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Dialog Summarization í‰ê°€ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í‰ê°€ (ê¶Œì¥)
  python evaluate.py --model_path outputs/models/best
  
  # íŠ¹ì • ëª¨ë¸ í‰ê°€
  python evaluate.py --model_path outputs/models/model_20250727_145129
  
  # ê¸°ì¡´ ì˜ˆì¸¡ íŒŒì¼ í‰ê°€
  python evaluate.py --pred_file outputs/predictions/test_pred.csv
  
  # ìë™ ëª¨ë¸ ì°¾ê¸° (best ëª¨ë¸ ìë™ ì„ íƒ)
  python evaluate.py --auto
        """
    )

    # í•˜ë‚˜ì˜ ì˜µì…˜ì´ë¼ë„ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ --auto ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ ì„¤ì •
    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument("--model_path", 
                       help="ëª¨ë¸ ê²½ë¡œ ì§€ì • (ì˜ˆ: outputs/models/best, outputs/models/latest)")
    group.add_argument("--pred_file", 
                       help="ê¸°ì¡´ ì˜ˆì¸¡ CSV íŒŒì¼ë¡œ í‰ê°€ (ëª¨ë¸ ë¡œë“œ ì—†ì´ ë°”ë¡œ í‰ê°€)")
    group.add_argument("--auto", action="store_true", 
                       help="ìë™ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ (ê°€ì¥ ê°„í¸í•œ ë°©ë²•)")

    parser.add_argument("--ref_file", default=None, 
                       help="ì •ë‹µ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: data/processed/test.csv)")
    parser.add_argument("--output_file", default="outputs/predictions/test_pred.csv",
                       help="ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: outputs/predictions/test_pred.csv)")

    args = parser.parse_args()

    # ì¸ìˆ˜ê°€ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ìë™ ëª¨ë“œë¡œ ì „í™˜
    if not (args.auto or args.model_path or args.pred_file):
        args.auto = True
        print("âš™ï¸  ì¸ìˆ˜ê°€ ì—†ì–´ì„œ ìë™ ëª¨ë“œ(--auto)ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    # ref_file ê¸°ë³¸ê°’ (ê°€ì¥ ë¨¼ì € ì²˜ë¦¬)
    if args.ref_file is None:
        project_root = Path(__file__).resolve().parent.parent.parent
        cfg_path = project_root / "src" / "config" / "config.yaml"
        with open(cfg_path, "r") as f:
            _cfg = yaml.safe_load(f)
        version = _cfg.get('general', {}).get('preprocess_version', 'v1')
        args.ref_file = f"data/processed/{version}/dev.csv"

    try:
        # 1. ëª¨ë¸ ê²½ë¡œ ê²°ì •
        if args.auto:
            model_path = find_best_model_path()
            if model_path is None:
                print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ê±°ë‚˜ ì •í™•í•œ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.")
                return
            print(f"ğŸ” ìë™ ì„ íƒëœ ëª¨ë¸: {model_path}")
            pred_path = generate_predictions(model_path, args.ref_file, args.output_file)
            model_info = f"({os.path.basename(model_path)} ëª¨ë¸)"
            
        elif args.model_path:
            if not os.path.exists(args.model_path):
                print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model_path}")
                return
            pred_path = generate_predictions(args.model_path, args.ref_file, args.output_file)
            model_info = f"({os.path.basename(args.model_path)} ëª¨ë¸)"
            
        else:  # args.pred_file
            pred_path = args.pred_file
            model_info = "(ê¸°ì¡´ ì˜ˆì¸¡ íŒŒì¼)"

        # 2. í‰ê°€ ìˆ˜í–‰
        scores = evaluate_predictions(pred_path, args.ref_file)
        
        # 3. ê²°ê³¼ ì¶œë ¥
        print_results(scores, model_info)

        # ìµœì‹  test_pred.csv ì‹¬í”Œ íŒŒì¼ ì—…ë°ì´íŠ¸
        import shutil, os as _os
        if _os.path.basename(pred_path).startswith("test_pred"):
            simple_path = _os.path.join(_os.path.dirname(pred_path), "test_pred.csv")
            shutil.copy(pred_path, simple_path)
            print(f"ğŸ“‹ {simple_path} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
         
        return scores

    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        return None


if __name__ == "__main__":
    main()
