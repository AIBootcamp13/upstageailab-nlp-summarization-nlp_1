"""
BART ê¸°ë°˜ ëŒ€í™” ìš”ì•½ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
train.pyì˜ ë°ì´í„° ë¡œë”© ë°©ì‹ì„ ë°˜ì˜í•˜ì—¬ ë¦¬íŒ©í† ë§
"""

import pandas as pd
import os
import sys
import yaml
import torch
import argparse
from typing import Optional
from tqdm import tqdm
from transformers import (
    AutoTokenizer, 
    BartForConditionalGeneration,
    DataCollatorForSeq2Seq, # ë°ì´í„° ì½œë ˆì´í„° ì¶”ê°€
)
from pathlib import Path
from datasets import load_dataset, Dataset # datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ data ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = Path(current_dir).parent.parent
sys.path.append(str(project_root / "src"))

from utils.postprocess import postprocess


# =========================================================================
# ì¶”ë¡ ìš© ë°ì´í„°ì…‹ ì¤€ë¹„ í•¨ìˆ˜ (train.py ìŠ¤íƒ€ì¼ë¡œ ë¦¬íŒ©í† ë§)
# =========================================================================

def prepare_test_dataset(config, tokenizer):
    test_file_path = os.path.join(project_root, config['test_file'])
    
    # HuggingFace datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ CSV ë¡œë“œ
    raw_dataset = load_dataset('csv', data_files={'test': test_file_path})
    test_dataset = raw_dataset['test']
    
    print('-' * 150)
    print(f'test_data (from datasets):\n{test_dataset[0]["dialogue"]}')
    print('-' * 150)

    def tokenize_function(examples):
        # ëª¨ë¸ ì…ë ¥(ì¸ì½”ë”) í† í¬ë‚˜ì´ì§• (ë ˆì´ë¸” ì—†ìŒ)
        model_inputs = tokenizer(
            examples['dialogue'],
            max_length=config['tokenizer']['encoder_max_len'],
            truncation=True,
            padding='max_length'
        )
        return model_inputs

    # map í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ì „ì²´ì— í† í¬ë‚˜ì´ì§• ì ìš©
    tokenized_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)
    
    print('-' * 10, 'Make dataset complete', '-' * 10)
    return test_dataset, tokenized_dataset # ì›ë³¸ê³¼ í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹ ë°˜í™˜


# =========================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ê±°ì˜ ë™ì¼, ì¼ë¶€ ì •ë¦¬)
# =========================================================================

def load_tokenizer_and_model_for_test(config, device, model_path=None):
    print('-' * 10, 'Load tokenizer & model', '-' * 10)
    
    model_name = config['general']['model_name']
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}
    tokenizer.add_special_tokens(special_tokens_dict)

    if model_path:
        print('-' * 10, f'Using trained model: {model_path}', '-' * 10)
        generate_model = BartForConditionalGeneration.from_pretrained(model_path)
    else:
        print('-' * 10, f'Using base model: {model_name}', '-' * 10)
        generate_model = BartForConditionalGeneration.from_pretrained(model_name)
        
    generate_model.resize_token_embeddings(len(tokenizer))
    generate_model.to(device)
    print('-' * 10, 'Load tokenizer & model complete', '-' * 10)

    return generate_model, tokenizer


# =========================================================================
# ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜ (ë¦¬íŒ©í† ë§)
# =========================================================================

def inference(config, model_path=None, sample_size=None):
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print('-' * 10, f'device : {device}', '-' * 10)
    print(f'PyTorch version: {torch.__version__}')

    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device, model_path)

    # ë°ì´í„°ì…‹ ì¤€ë¹„ (ìƒˆë¡œìš´ í•¨ìˆ˜ í˜¸ì¶œ)
    original_test_dataset, tokenized_test_dataset = prepare_test_dataset(config, tokenizer)
    
    # ìƒ˜í”Œ í¬ê¸° ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    if sample_size:
        print(f"ğŸ”§ {sample_size}ê°œ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
        original_test_dataset = original_test_dataset.select(range(sample_size))
        tokenized_test_dataset = tokenized_test_dataset.select(range(sample_size))

    # ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
    data_collator = DataCollatorForSeq2Seq(tokenizer, model=generate_model)

    # DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬
    dataloader = torch.utils.data.DataLoader(
        tokenized_test_dataset,
        batch_size=config['inference']['batch_size'],
        collate_fn=data_collator,
        shuffle=False
    )
    
    summary = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="ìš”ì•½ ìƒì„±"):
            # í•„ìš”í•œ ë°ì´í„°ë§Œ deviceë¡œ ì´ë™ (ê°’ì´ Tensorì¸ ê²½ìš°ì—ë§Œ)
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            
            gen_kwargs = {
                'max_length': config['inference'].get('generation_max_length', 128),
                'num_beams': config['inference']['num_beams'],
                'length_penalty': config['inference'].get('length_penalty', 1.0),
                'early_stopping': config['inference'].get('early_stopping', True),
            }
            # train.pyì˜ generation_max_lengthì™€ ì¼ì¹˜ì‹œí‚¤ê±°ë‚˜, infer ì„¤ì • ë”°ë¡œ ê´€ë¦¬
            
            generated_ids = generate_model.generate(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                **gen_kwargs
            )
            
            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            
            # í›„ì²˜ë¦¬ ì ìš©
            postprocessed_preds = [postprocess(pred, config['inference']['remove_tokens']) for pred in decoded_preds]
            summary.extend(postprocessed_preds)

    # ê²°ê³¼ë¬¼ DataFrame ìƒì„±
    output = pd.DataFrame({
        "fname": original_test_dataset['fname'],
        "summary": summary,
    })
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_file = config.get('output_file', 'outputs/predictions/submission.csv')
    # submission íŒŒì¼ì¼ ê²½ìš° ëª¨ë¸ ì •ë³´(ë””ë ‰í† ë¦¬ëª…) ì¶”ê°€
    if os.path.basename(output_file).startswith('submission'):
        model_dir_name = os.path.basename(os.path.realpath(model_path)) if model_path else 'base'
        name, ext = os.path.splitext(output_file)
        output_file = f"{name}_{model_dir_name}{ext}"
    result_path = os.path.dirname(output_file)
    os.makedirs(result_path, exist_ok=True)
    
    # ë¦¬ë”ë³´ë“œ ì œì¶œ(sample_submission.csv) í˜•ì‹: ì²« ë²ˆì§¸ì— ì¸ë±ìŠ¤ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆìŒ
    # submission*.csv íŒŒì¼ì¼ ë•Œë§Œ index=True ë¡œ ì €ì¥í•˜ê³ , dev_pred ë“±ì€ ê¸°ì¡´ì²˜ëŸ¼ index ì œì™¸
    include_index = os.path.basename(output_file).startswith("submission")
    output.to_csv(output_file, index=include_index)
    print(f"âœ… ì¶”ë¡  ì™„ë£Œ! ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    print(f"ğŸ“ˆ ì´ {len(summary)}ê°œ ì˜ˆì¸¡ ìƒì„±ë¨")

    # ìµœì‹  submission.csv ì—…ë°ì´íŠ¸
    import shutil, os as _os
    if _os.path.basename(output_file).startswith("submission"):
        simple_sub = _os.path.join(_os.path.dirname(output_file), "submission.csv")
        shutil.copy(output_file, simple_sub)
        print(f"ğŸ“‹ {simple_sub} ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    return output


# =========================================================================
# ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ íƒìƒ‰ í•¨ìˆ˜ (ì œê±°ë¨ - run_all.shì—ì„œ ì²˜ë¦¬)
# =========================================================================

def main(model_path, config_path=None, sample_size=None): # model_pathë¥¼ í•„ìˆ˜ ì¸ìë¡œ ë³€ê²½
    # Config íŒŒì¼ ê²½ë¡œ ì„¤ì •
    if config_path is None:
        config_path = os.path.join(project_root, "src", "config", "config_baseline.yaml")
    
    # Config íŒŒì¼ ë¡œë“œ
    with open(config_path, "r") as file:
        config = yaml.safe_load(file)

    # --- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ê²€ìƒ‰ ë¡œì§ (ì œê±°ë¨) ---
    if not model_path:
        raise ValueError("--model_path ì¸ìëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

    # ì¶”ë¡  ê´€ë ¨ ê¸°ë³¸ ì„¤ì • ì¶”ê°€ (config.yamlì— ëª…ì‹œí•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŒ)
    if 'test_file' not in config:
        version = config['general'].get('preprocess_version', 'v1')
        config['test_file'] = f'data/processed/{version}/test.csv'
    if 'output_file' not in config:
        config['output_file'] = 'outputs/predictions/submission.csv'
    
    return inference(config, model_path, sample_size)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="BART ëŒ€í™” ìš”ì•½ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--model_path", help="ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: outputs/models/best, outputs/models/latest)")
    parser.add_argument("--config_path", default="src/config/config.yaml", help="Config YAML ê²½ë¡œ")
    parser.add_argument("--sample_size", type=int, help="í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê°œìˆ˜(ì„ íƒ)")

    args = parser.parse_args()

    # model_pathê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ best -> latest ìˆœìœ¼ë¡œ ì„ íƒ
    def _auto_model_path(models_dir: str = "outputs/models") -> Optional[str]:
        best_link = os.path.join(models_dir, "best")
        if os.path.exists(best_link):
            return best_link
        latest_link = os.path.join(models_dir, "latest")
        if os.path.exists(latest_link):
            print("âš ï¸ 'best' ëª¨ë¸ì´ ì—†ì–´ 'latest' ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return latest_link
        return None

    model_path = args.model_path
    if model_path is None:
        model_path = _auto_model_path()
        if model_path is None:
            print("âŒ model_pathê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ê³  ìë™ìœ¼ë¡œ ì‚¬ìš©í•  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        print(f"ğŸ” ìë™ ì„ íƒëœ ëª¨ë¸: {model_path}")

    main(model_path=model_path, config_path=args.config_path, sample_size=args.sample_size)
