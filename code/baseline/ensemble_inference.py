#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Ensemble Inference System for Multiple Strategies
WandB sweepìœ¼ë¡œ ìƒì„±ëœ ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ì‹œìŠ¤í…œ

ì§€ì›í•˜ëŠ” ì•™ìƒë¸” ë°©ì‹:
1. í•˜ë“œ ë³´íŒ… (Hard Voting): í† í°ë³„ ë‹¤ìˆ˜ê²°
2. ì†Œí”„íŠ¸ ë³´íŒ… (Soft Voting): í™•ë¥  ë¶„í¬ í‰ê·   
3. ê¸¸ì´ ê¸°ë°˜ (Length-based): ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
4. ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” (Realtime Token Ensemble): ë§¤ í† í°ë§ˆë‹¤ í™•ë¥  ë¶„í¬ í‰ê· 

ì‚¬ìš©ë²•:
- python ensemble_inference.py --mode=all           # ëª¨ë“  ë°©ì‹ ë¹„êµ
- python ensemble_inference.py --mode=hard_voting   # í•˜ë“œ ë³´íŒ…ë§Œ
- python ensemble_inference.py --mode=soft_voting   # ì†Œí”„íŠ¸ ë³´íŒ…ë§Œ
- python ensemble_inference.py --mode=length_based  # ê¸¸ì´ ê¸°ë°˜ë§Œ
- python ensemble_inference.py --mode=realtime_token # ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”ë§Œ
"""

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
import os; os.chdir(os.path.dirname(os.path.abspath(__file__)))
import sys; sys.path.append('../utils')
import log_util as log

import pandas as pd
import json
import yaml
import torch
import zipfile
import shutil
import time
from datetime import datetime
from collections import Counter
from tqdm import tqdm
import random
import numpy as np

from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig

# baseline.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ ì„í¬íŠ¸
from baseline import Preprocess, DatasetForVal, compute_metrics
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import tempfile

def get_model_paths():
    """
    ëª¨ë¸ ê²½ë¡œë“¤ì„ ë°˜í™˜í•˜ëŠ” ê³µí†µ í•¨ìˆ˜
    
    Returns:
        list: ì¡´ì¬í•˜ëŠ” ëª¨ë¸ íŒŒì¼ ê²½ë¡œë“¤
    """
    # TODO: ì‹¤ì œ ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”
    model_paths = [
        "./models/model_baseline_20250804_063540.zip",  
        "./models/model_baseline_20250804_064025.zip",
    ]
    
    # ì¡´ì¬í•˜ëŠ” ëª¨ë¸ íŒŒì¼ë§Œ í•„í„°ë§
    existing_model_paths = []
    for path in model_paths:
        if os.path.exists(path):
            existing_model_paths.append(path)
            log.info(f"ëª¨ë¸ íŒŒì¼ í™•ì¸: {path}")
        else:
            log.warning(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ (ê±´ë„ˆëœ€): {path}")
    
    if not existing_model_paths:
        log.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        log.info("ë¨¼ì € WandB sweepì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ì„¸ìš”:")
        log.info("python wandb_sweep.py --count 3")
        return []
    
    log.info(f"ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨")
    return existing_model_paths

def load_model_package(zip_path):
    """
    ZIP íŒŒì¼ì—ì„œ ëª¨ë¸, í† í¬ë‚˜ì´ì €, ì„¤ì •ì„ ë¡œë”©
    
    Args:
        zip_path: ZIP íŒŒì¼ ê²½ë¡œ
        
    Returns:
        tuple: (model, tokenizer, config, metadata)
    """
    temp_dir = f"temp_load_{int(time.time())}"
    
    try:
        log.info(f"ëª¨ë¸ íŒ¨í‚¤ì§€ ë¡œë”© ì‹œì‘: {zip_path}")
        
        # ZIP ì••ì¶• í•´ì œ
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        
        # ì„¤ì • ë¡œë“œ
        config_path = os.path.join(temp_dir, "config.yaml")
        with open(config_path, "r", encoding='utf-8') as f:
            config = yaml.safe_load(f)
        log.info("ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = os.path.join(temp_dir, "metadata.json")
        with open(metadata_path, "r", encoding='utf-8') as f:
            metadata = json.load(f)
        log.info("ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # ì‹œë“œ ì„¤ì • (ì¬í˜„ì„± ë³´ì¥)
        if 'training' in config and 'seed' in config['training']:
            seed = config['training']['seed']
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            np.random.seed(seed)
            random.seed(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            log.info(f"ëª¨ë¸ ë¡œë”© ì‹œ ì‹œë“œ ì„¤ì •: {seed}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ ë¨¼ì € (ì‹¤ì œ vocab í¬ê¸° í™•ì¸ìš©)
        tokenizer_dir = os.path.join(temp_dir, "tokenizer")
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        
        # Special tokens ì¶”ê°€ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
        if 'tokenizer' in config and 'special_tokens' in config['tokenizer']:
            special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}
            tokenizer.add_special_tokens(special_tokens_dict)
            log.info(f"Special tokens ì¶”ê°€: {config['tokenizer']['special_tokens']}")
        
        log.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        
        # ì €ì¥ëœ ëª¨ë¸ì˜ config.json ì§ì ‘ ë¡œë“œ ë° ìˆ˜ì •
        try:
            # ì €ì¥ëœ config.json íŒŒì¼ ì½ê¸°
            config_path = os.path.join(temp_dir, "config.json")
            if os.path.exists(config_path):
                with open(config_path, "r", encoding='utf-8') as f:
                    model_config_dict = json.load(f)
                
                # ë¶„ë¥˜ ê´€ë ¨ ì„¤ì • ì œê±°
                model_config_dict.pop('num_labels', None)
                model_config_dict.pop('id2label', None)
                model_config_dict.pop('label2id', None)
                
                # ìˆ˜ì •ëœ configë¡œ BartConfig ìƒì„±
                bart_config = BartConfig(**model_config_dict)
                log.info(f"BART ì„¤ì • ë¡œë“œ ì™„ë£Œ (config.json ì‚¬ìš©), vocab_size: {bart_config.vocab_size}")
            else:
                # config.jsonì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
                model_name = config['general']['model_name']
                bart_config = BartConfig.from_pretrained(model_name)
                actual_vocab_size = len(tokenizer)
                bart_config.vocab_size = actual_vocab_size
                log.info(f"BART ì„¤ì • ë¡œë“œ ì™„ë£Œ (ê¸°ë³¸ ë°©ì‹), vocab_size: {actual_vocab_size}")
        except Exception as e:
            log.warning(f"config.json ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
            model_name = config['general']['model_name']
            bart_config = BartConfig.from_pretrained(model_name)
            actual_vocab_size = len(tokenizer)
            bart_config.vocab_size = actual_vocab_size
        
        # ëª¨ë¸ ë¡œë“œ (configì˜ vocab_sizeê°€ ì¡°ì •ëœ ìƒíƒœ)
        model = BartForConditionalGeneration.from_pretrained(temp_dir, config=bart_config)
        
        # í† í° ì„ë² ë”© í¬ê¸° ì¡°ì • (í•„ìˆ˜! wandb_sweep.pyì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
        # special tokensê°€ ì¶”ê°€ëœ ê²½ìš° ë°˜ë“œì‹œ í•„ìš”
        model.resize_token_embeddings(len(tokenizer))
        model.eval()  # evaluation ëª¨ë“œ ì„¤ì •
        log.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        return model, tokenizer, config, metadata
        
    except Exception as e:
        log.error(f"ëª¨ë¸ íŒ¨í‚¤ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        log.error(f"ì˜¤ë¥˜ ì„¸ë¶€ ì •ë³´: {type(e).__name__}")
        if "num_labels" in str(e) and "id2label" in str(e):
            log.error("íˆíŠ¸: BART ëª¨ë¸ ì„¤ì • ë¶ˆì¼ì¹˜ ë¬¸ì œì…ë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ê±°ë‚˜ config ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        raise
        
    finally:
        # ì„ì‹œ í´ë” ì‚­ì œ
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)

def prepare_validation_dataset_for_ensemble(config, preprocessor, tokenizer):
    """
    baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        preprocessor: ë°ì´í„° ì „ì²˜ë¦¬ê¸°
        tokenizer: í† í¬ë‚˜ì´ì €
        
    Returns:
        DatasetForVal: ê²€ì¦ ë°ì´í„°ì…‹
    """
    log.info("ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘ (baseline.py ë°©ì‹)")
    
    # ê²€ì¦ ë°ì´í„° ë¡œë“œ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    data_path = config['general']['data_path']
    val_file_path = os.path.join(data_path, 'dev.csv')
    val_data = preprocessor.make_set_as_df(val_file_path)
    
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)
    
    # í† í¬ë‚˜ì´ì € ì ìš© (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    val_tokenized_encoder_inputs = tokenizer(
        encoder_input_val, 
        return_tensors="pt", 
        padding=True,
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['encoder_max_len'], 
        return_token_type_ids=False
    )
    val_tokenized_decoder_inputs = tokenizer(
        decoder_input_val, 
        return_tensors="pt", 
        padding=True,
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['decoder_max_len'], 
        return_token_type_ids=False
    )
    val_tokenized_decoder_outputs = tokenizer(
        decoder_output_val, 
        return_tensors="pt", 
        padding=True,
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['decoder_max_len'], 
        return_token_type_ids=False
    )
    
    # baseline.pyì™€ ë™ì¼í•œ DatasetForVal í´ë˜ìŠ¤ ì‚¬ìš©
    val_inputs_dataset = DatasetForVal(
        val_tokenized_encoder_inputs, 
        val_tokenized_decoder_inputs, 
        val_tokenized_decoder_outputs,
        len(encoder_input_val)
    )
    
    log.info("ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    return val_inputs_dataset

def evaluate_single_model_with_baseline(model, tokenizer, config):
    """
    baseline.py ë°©ì‹ìœ¼ë¡œ ë‹¨ì¼ ëª¨ë¸ ê²€ì¦ ì ìˆ˜ ê³„ì‚°
    
    Args:
        model: ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì •
        
    Returns:
        dict: ROUGE ë©”íŠ¸ë¦­ ê²°ê³¼
    """
    log.info("baseline.py ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
    
    # ë°ì´í„° ì „ì²˜ë¦¬ê¸° ìƒì„± (baseline.pyì™€ ë™ì¼)
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])
    
    # ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    val_inputs_dataset = prepare_validation_dataset_for_ensemble(config, preprocessor, tokenizer)
    
    # Seq2SeqTrainingArguments ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°, wandb ë¹„í™œì„±í™”)
    temp_dir = tempfile.mkdtemp()
    training_args = Seq2SeqTrainingArguments(
        output_dir=temp_dir,
        predict_with_generate=config['training']['predict_with_generate'],
        generation_max_length=config['training']['generation_max_length'],
        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
        seed=config['training']['seed'],
        report_to=[],  # wandb ë¹„í™œì„±í™”
        logging_strategy="no",  # ë¡œê¹… ë¹„í™œì„±í™”
    )
    
    # compute_metrics í•¨ìˆ˜ë¥¼ ìœ„í•œ wrapper (baseline.pyì™€ ë™ì¼)
    def compute_metrics_wrapper(pred):
        return compute_metrics(config, tokenizer, pred)
    
    # Seq2SeqTrainer ìƒì„± (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        eval_dataset=val_inputs_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics_wrapper
    )
    
    # í‰ê°€ ìˆ˜í–‰ (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    log.info("Seq2SeqTrainerë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹œì‘")
    eval_results = trainer.evaluate()
    log.info("í‰ê°€ ì™„ë£Œ")
    
    # ê²°ê³¼ ì¶”ì¶œ
    rouge_results = {}
    for key, value in eval_results.items():
        if 'rouge' in key and key != 'eval_rouge_avg':
            metric_name = key.replace('eval_', '')
            rouge_results[metric_name] = value
    
    # rouge-avg ê³„ì‚° ì¶”ê°€
    if 'rouge-1' in rouge_results and 'rouge-2' in rouge_results and 'rouge-l' in rouge_results:
        rouge_avg = (rouge_results['rouge-1'] + rouge_results['rouge-2'] + rouge_results['rouge-l']) / 3
        rouge_results['rouge-avg'] = rouge_avg
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    return rouge_results

class RealtimeTokenEnsemble:
    """
    ì‹¤ì‹œê°„ í† í° ë‹¨ìœ„ ì•™ìƒë¸” í´ë˜ìŠ¤
    ê° ìŠ¤í…ë§ˆë‹¤ ëª¨ë“  ëª¨ë¸ì—ì„œ ë‹¤ìŒ í† í° í™•ë¥  ë¶„í¬ë¥¼ íšë“í•˜ì—¬ ì•™ìƒë¸”
    """
    
    def __init__(self, model_paths, device="cuda:0"):
        """
        Args:
            model_paths: ëª¨ë¸ ZIP íŒŒì¼ ê²½ë¡œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.model_paths = model_paths
        self.device = device
        self.models = []
        self.tokenizers = []
        self.configs = []
        self.metadata_list = []
        
        log.info(f"ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {len(model_paths)}ê°œ ëª¨ë¸")
        log.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ë“¤ì„ ë¡œë”©"""
        log.info("ëª¨ë¸ë“¤ ë¡œë”© ì‹œì‘...")
        
        for i, path in enumerate(self.model_paths):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.model_paths)} ë¡œë”© ì¤‘: {path}")
            
            try:
                model, tokenizer, config, metadata = load_model_package(path)
                
                # GPU ë©”ëª¨ë¦¬ í™•ì¸ ë° ëª¨ë¸ ë¡œë”©
                try:
                    model.to(self.device)
                    model.eval()
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        log.error(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨. CPUë¡œ fallback ì‹œë„...")
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                        self.device = "cpu"
                        model.to(self.device)
                        model.eval()
                        log.warning(f"ëª¨ë¸ {i+1}ì„ CPUì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì„±ëŠ¥ì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        raise
                
                self.models.append(model)
                self.tokenizers.append(tokenizer)
                self.configs.append(config)
                self.metadata_list.append(metadata)
                
                log.info(f"ëª¨ë¸ {i+1} ë¡œë”© ì™„ë£Œ: {metadata.get('wandb_run_name', 'Unknown')} (device: {self.device})")
                
            except Exception as e:
                log.error(f"ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨: {e}")
                log.error(f"ê²½ë¡œ: {path}")
                raise
        
        log.info(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def generate_ensemble_sequence_single(self, input_text, config):
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” (ê°œì„ ëœ ë¡œì§)
        
        Args:
            input_text: ë‹¨ì¼ ì…ë ¥ í…ìŠ¤íŠ¸
            config: ìƒì„± ì„¤ì •
            
        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        tokenizer = self.tokenizers[0]
        max_length = config['inference']['generate_max_length']
        
        # ì…ë ¥ í† í°í™”
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            max_length=config['tokenizer']['encoder_max_len'],
            truncation=True,
            padding=True
        ).to(self.device)
        
        # ğŸš€ í•µì‹¬ ìµœì í™”: ê° ëª¨ë¸ì˜ encoder outputì„ í•œ ë²ˆë§Œ ê³„ì‚°
        model_encoder_outputs = []
        for model in self.models:
            with torch.no_grad():
                encoder_outputs = model.get_encoder()(
                    input_ids=inputs['input_ids'], 
                    attention_mask=inputs['attention_mask']
                )
                model_encoder_outputs.append(encoder_outputs.last_hidden_state)
        
        # ë””ì½”ë” ì‹œì‘ í† í°
        decoder_start_token_id = tokenizer.bos_token_id
        if decoder_start_token_id is None:
            decoder_start_token_id = tokenizer.eos_token_id
        
        # ìƒì„±ëœ ì‹œí€€ìŠ¤ (ì‹œì‘ í† í°ìœ¼ë¡œ ì´ˆê¸°í™”)
        generated_sequence = [decoder_start_token_id]
        eos_token_id = tokenizer.eos_token_id
        
        # ğŸ”„ í† í°ë³„ ìƒì„± ë£¨í”„
        for step in range(max_length - 1):
            # í˜„ì¬ê¹Œì§€ì˜ ì‹œí€€ìŠ¤ë¥¼ í…ì„œë¡œ ë³€í™˜
            current_ids = torch.tensor([generated_sequence], device=self.device)
            
            # ê° ëª¨ë¸ì—ì„œ ë‹¤ìŒ í† í° logits ê³„ì‚°
            model_logits = []
            successful_models = 0
            
            for i, model in enumerate(self.models):
                try:
                    with torch.no_grad():
                        # ë””ì½”ë” ì‹¤í–‰ (ë¯¸ë¦¬ ê³„ì‚°ëœ encoder output ì‚¬ìš©)
                        decoder_outputs = model.get_decoder()(
                            input_ids=current_ids,
                            encoder_hidden_states=model_encoder_outputs[i],
                            encoder_attention_mask=inputs['attention_mask']
                        )
                        
                        # LM headë¡œ vocabulary logits ê³„ì‚°
                        logits = model.lm_head(decoder_outputs.last_hidden_state)
                        next_token_logits = logits[0, -1, :]  # ë§ˆì§€ë§‰ ìœ„ì¹˜ì˜ logits
                        
                        model_logits.append(next_token_logits)
                        successful_models += 1
                        
                except Exception as e:
                    log.warning(f"ëª¨ë¸ {i+1} ìŠ¤í… {step} ì˜¤ë¥˜: {e}")
                    continue
            
            if successful_models == 0:
                log.error(f"ìŠ¤í… {step}: ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨")
                break
            
            # ğŸ§® ì„±ê³µí•œ ëª¨ë¸ë“¤ì˜ logits í‰ê·  ê³„ì‚°
            if len(model_logits) > 1:
                ensemble_logits = torch.stack(model_logits).mean(dim=0)
            else:
                ensemble_logits = model_logits[0]
            
            # ğŸ¯ Greedy decoding: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
            next_token_id = torch.argmax(ensemble_logits).item()
            
            # ìƒì„±ëœ í† í°ì„ ì‹œí€€ìŠ¤ì— ì¶”ê°€
            generated_sequence.append(next_token_id)
            
            # âœ… EOS í† í° ë„ë‹¬ ì‹œ ìƒì„± ì¢…ë£Œ
            if next_token_id == eos_token_id:
                log.debug(f"EOS ë„ë‹¬: ìŠ¤í… {step}, ê¸¸ì´ {len(generated_sequence)}")
                break
        
        # ğŸ”¤ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)
        
        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        for token in config['inference']['remove_tokens']:
            generated_text = generated_text.replace(token, " ")
            
        return generated_text.strip()
    
    def generate_ensemble_sequence(self, input_ids, config):
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ (í•˜ìœ„ í˜¸í™˜ì„±)
        
        Args:
            input_ids: ì…ë ¥ í† í° ID
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬  
            config: ìƒì„± ì„¤ì •
            
        Returns:
            torch.Tensor: ìƒì„±ëœ ì‹œí€€ìŠ¤
        """
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ìœ„ì„
        tokenizer = self.tokenizers[0]
        input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
        generated_text = self.generate_ensemble_sequence_single(input_text, config)
        
        # ë‹¤ì‹œ í† í°í™”í•˜ì—¬ ë°˜í™˜
        generated_ids = tokenizer(
            generated_text, 
            return_tensors="pt",
            add_special_tokens=False
        )['input_ids']
        
        return generated_ids
    
    def generate_with_realtime_ensemble(self, input_texts, config):
        """
        ì‹¤ì‹œê°„ ì•™ìƒë¸”ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ìƒì„± ì„¤ì •
            
        Returns:
            list: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        log.info(f"Realtime Token Ensemble ì‹œì‘: {len(input_texts)}ê°œ í…ìŠ¤íŠ¸")
        
        for i, text in enumerate(tqdm(input_texts, desc="ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘")):
            try:
                # ğŸš€ ê°œì„ ëœ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‚¬ìš©
                generated_text = self.generate_ensemble_sequence_single(text, config)
                results.append(generated_text)
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (ë§¤ 10ê°œë§ˆë‹¤)
                if (i + 1) % 10 == 0:
                    log.info(f"ì§„í–‰ ìƒí™©: {i+1}/{len(input_texts)} ì™„ë£Œ")
                
            except Exception as e:
                log.warning(f"í…ìŠ¤íŠ¸ {i+1} ì‹¤ì‹œê°„ ì•™ìƒë¸” ì˜¤ë¥˜: {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        log.info("Realtime Token Ensemble ì™„ë£Œ")
        return results
    
    def generate_with_single_model(self, model, tokenizer, config, input_texts):
        """
        ë¹„êµë¥¼ ìœ„í•œ ë‹¨ì¼ ëª¨ë¸ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            model: ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €  
            config: ì„¤ì •
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for text in tqdm(input_texts, desc="ë‹¨ì¼ ëª¨ë¸ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘"):
            try:
                inputs = tokenizer(
                    text, 
                    return_tensors="pt", 
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                
                # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                for token in config['inference']['remove_tokens']:
                    generated_text = generated_text.replace(token, " ")
                
                results.append(generated_text.strip())
                
            except Exception as e:
                log.warning(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        return results
    
    def evaluate_on_validation(self, val_data_path):
        """
        ì‹¤ì‹œê°„ ì•™ìƒë¸” ê²€ì¦ ë°ì´í„° í‰ê°€
        
        Args:
            val_data_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            dict: í‰ê°€ ê²°ê³¼
        """
        import time
        
        log.info(f"Realtime Token Ensemble ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘: {val_data_path}")
        
        # ê²€ì¦ ë°ì´í„° ë¡œë“œ
        try:
            val_df = pd.read_csv(val_data_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            required_columns = ['dialogue', 'summary']
            for col in required_columns:
                if col not in val_df.columns:
                    log.error(f"ê²€ì¦ ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ '{col}'ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(val_df.columns)}")
                    return None
            
            val_df_sample = val_df.head(50)  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 50ê°œë§Œ
            input_texts = val_df_sample['dialogue'].tolist()
            reference_summaries = val_df_sample['summary'].tolist()
            
            # ë¹ˆ ë°ì´í„° í™•ì¸
            if not input_texts or not reference_summaries:
                log.error("ê²€ì¦ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None
                
            log.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(input_texts)}ê°œ ìƒ˜í”Œ")
        except FileNotFoundError:
            log.error(f"ê²€ì¦ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_data_path}")
            return None
        except pd.errors.EmptyDataError:
            log.error(f"ê²€ì¦ ë°ì´í„° íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {val_data_path}")
            return None
        except Exception as e:
            log.error(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
        
        # ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        # Realtime Token Ensembleìœ¼ë¡œ ìƒì„±
        log.info("Realtime Token Ensemble ìƒì„± ì‹œì‘...")
        realtime_results = self.generate_with_realtime_ensemble(input_texts, self.configs[0])
        
        generation_time = time.time() - start_time
        log.info(f"Realtime Token Ensemble ìƒì„± ì™„ë£Œ: {generation_time:.2f}ì´ˆ")
        
        # ROUGE ì ìˆ˜ ê³„ì‚°
        def calculate_rouge_scores(predictions, references, method_name):
            from rouge import Rouge
            rouge = Rouge()
            
            # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
            cleaned_predictions = []
            cleaned_references = []
            for pred, ref in zip(predictions, references):
                pred_clean = pred.strip()
                ref_clean = ref.strip()
                for token in self.configs[0]['inference']['remove_tokens']:
                    pred_clean = pred_clean.replace(token, " ")
                    ref_clean = ref_clean.replace(token, " ")
                pred_clean = pred_clean.strip() if pred_clean.strip() else "empty"
                ref_clean = ref_clean.strip() if ref_clean.strip() else "empty"
                cleaned_predictions.append(pred_clean)
                cleaned_references.append(ref_clean)
            
            try:
                rouge_results = rouge.get_scores(cleaned_predictions, cleaned_references, avg=True)
                rouge_scores = {key: value["f"] for key, value in rouge_results.items()}
                # rouge-avg ê³„ì‚° ì¶”ê°€
                rouge_avg = (rouge_scores['rouge-1'] + rouge_scores['rouge-2'] + rouge_scores['rouge-l']) / 3
                rouge_scores['rouge-avg'] = rouge_avg
                
                log.info(f"{method_name} ê²€ì¦ ì ìˆ˜ - ROUGE-1: {rouge_scores['rouge-1']:.4f}, "
                        f"ROUGE-2: {rouge_scores['rouge-2']:.4f}, ROUGE-L: {rouge_scores['rouge-l']:.4f}, "
                        f"ROUGE-avg: {rouge_scores['rouge-avg']:.4f}")
                return rouge_scores
            except Exception as e:
                log.warning(f"{method_name} ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
                return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'rouge-avg': 0.0}
        
        realtime_scores = calculate_rouge_scores(realtime_results, reference_summaries, "Realtime Token Ensemble")
        
        evaluation_results = {
            'realtime_token_ensemble_scores': realtime_scores,
            'generation_time_seconds': generation_time,
            'num_validation_samples': len(input_texts)
        }
        
        log.info("Realtime Token Ensemble ê²€ì¦ ë°ì´í„° í‰ê°€ ì™„ë£Œ")
        return evaluation_results
    
    def run_ensemble(self, test_data_path):
        """
        Realtime Token Ensemble ì‹¤í–‰
        
        Args:
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            tuple: (ensemble_result_df, generation_time)
        """
        import time
        
        log.info(f"Realtime Token Ensemble ì¶”ë¡  ì‹œì‘: {test_data_path}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        try:
            test_df = pd.read_csv(test_data_path)
            test_df_sample = test_df.head(20)  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
            input_texts = test_df_sample['dialogue'].tolist()
            log.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(input_texts)}ê°œ ìƒ˜í”Œ")
        except Exception as e:
            log.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, 0
        
        # ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        
        # Realtime Token Ensemble ì‹¤í–‰
        realtime_results = self.generate_with_realtime_ensemble(input_texts, self.configs[0])
        
        generation_time = time.time() - start_time
        log.info(f"Realtime Token Ensemble ì™„ë£Œ: {generation_time:.2f}ì´ˆ")
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        realtime_df = pd.DataFrame({
            'fname': test_df_sample['fname'],
            'summary': realtime_results
        })
        
        return realtime_df, generation_time

def main_comprehensive_experiment():
    """
    ğŸ”¬ ë„¤ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ì¢…í•© ë¹„êµ ì‹¤í—˜
    
    1. í•˜ë“œ ë³´íŒ… (Token-level Hard Voting)
    2. ì†Œí”„íŠ¸ ë³´íŒ… (Probability-based Soft Voting) 
    3. ê¸¸ì´ ê¸°ë°˜ (Length-based Selection)
    4. ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” (Realtime Token Ensemble)
    """
    import time
    
    log.info("ğŸ”¬ " + "="*60)
    log.info("ğŸ¯ ë„¤ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ì¢…í•© ë¹„êµ ì‹¤í—˜ ì‹œì‘")
    log.info("="*60)
    
    # ê³µí†µ í•¨ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    existing_model_paths = get_model_paths()
    if not existing_model_paths:
        log.error("ğŸ’¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    log.info(f"ğŸš€ ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ë¡œ ì‹¤í—˜ ì§„í–‰")
    
    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    val_data_path = "../../input/data/dev.csv"
    test_data_path = "../../input/data/test.csv"
    
    if not os.path.exists(val_data_path):
        log.error(f"ğŸ’¥ ê²€ì¦ ë°ì´í„° ì—†ìŒ: {val_data_path}")
        return
    if not os.path.exists(test_data_path):
        log.error(f"ğŸ’¥ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ: {test_data_path}")
        return
    
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ì‹¤í—˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    experiment_results = {
        'timestamp': timestamp,
        'model_paths': existing_model_paths,
        'device': device,
        'methods': {},
        'performance_ranking': [],
        'time_ranking': []
    }
    
    # ğŸ“Š ì‹¤í—˜ 1: HardVotingEnsembleì˜ ì„¸ ê°€ì§€ ë°©ì‹
    log.info("\n" + "ğŸ”¥ " + "="*50)
    log.info("ğŸ“Š ì‹¤í—˜ 1: Post-processing ì•™ìƒë¸” ë°©ì‹ë“¤")
    log.info("="*50)
    
    hard_ensemble = PostProcessingEnsemble(existing_model_paths, device=device)
    hard_ensemble.load_models()
    
    # ê²€ì¦ ë°ì´í„° í‰ê°€
    start_time = time.time()
    hard_evaluation = hard_ensemble.evaluate_on_validation(val_data_path)
    hard_time = time.time() - start_time
    
    if hard_evaluation:
        experiment_results['methods']['hard_voting'] = {
            'rouge_scores': hard_evaluation['hard_voting_scores'],
            'time_seconds': hard_time,
            'method_type': 'Post-processing'
        }
        experiment_results['methods']['soft_voting'] = {
            'rouge_scores': hard_evaluation['soft_voting_scores'],
            'time_seconds': hard_time,  # ê°™ì€ ì‹¤í–‰ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼
            'method_type': 'Post-processing'
        }
        experiment_results['methods']['length_based'] = {
            'rouge_scores': hard_evaluation['length_based_scores'],
            'time_seconds': hard_time,  # ê°™ì€ ì‹¤í–‰ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼  
            'method_type': 'Post-processing'
        }
    
    # ğŸ“Š ì‹¤í—˜ 2: RealtimeTokenEnsemble
    log.info("\n" + "ğŸ”¥ " + "="*50)
    log.info("âš¡ ì‹¤í—˜ 2: ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”")
    log.info("="*50)
    
    try:
        realtime_ensemble = RealtimeTokenEnsemble(existing_model_paths, device=device)
        realtime_ensemble.load_models()
        
        # ê²€ì¦ ë°ì´í„° í‰ê°€
        start_time = time.time()
        realtime_evaluation = realtime_ensemble.evaluate_on_validation(val_data_path)
        realtime_time = time.time() - start_time
        
        if realtime_evaluation:
            experiment_results['methods']['realtime_token_ensemble'] = {
                'rouge_scores': realtime_evaluation['realtime_token_ensemble_scores'],
                'time_seconds': realtime_time,
                'method_type': 'Runtime'
            }
    except Exception as e:
        log.error(f"ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        experiment_results['methods']['realtime_token_ensemble'] = {
            'error': str(e),
            'method_type': 'Runtime'
        }
    
    # ğŸ“ˆ ì„±ëŠ¥ ìˆœìœ„ ë¶„ì„
    log.info("\n" + "ğŸ† " + "="*50)
    log.info("ğŸ“ˆ ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼")
    log.info("="*50)
    
    # ROUGE-avg ê¸°ì¤€ ì„±ëŠ¥ ìˆœìœ„
    performance_data = []
    for method_name, method_data in experiment_results['methods'].items():
        if 'rouge_scores' in method_data:
            rouge_avg = method_data['rouge_scores']['rouge-avg']
            time_taken = method_data['time_seconds']
            performance_data.append((method_name, rouge_avg, time_taken))
    
    if performance_data:
        # ì„±ëŠ¥ìˆœ ì •ë ¬
        performance_data.sort(key=lambda x: x[1], reverse=True)
        experiment_results['performance_ranking'] = performance_data
        
        # ì†ë„ìˆœ ì •ë ¬
        time_data = sorted(performance_data, key=lambda x: x[2])
        experiment_results['time_ranking'] = time_data
        
        log.info("ğŸ¥‡ ì„±ëŠ¥ ìˆœìœ„ (ROUGE-avg ê¸°ì¤€):")
        for i, (method, rouge_avg, time_taken) in enumerate(performance_data, 1):
            method_type = experiment_results['methods'][method]['method_type']
            log.info(f"  {i}ìœ„. {method}: {rouge_avg:.4f} ({time_taken:.1f}ì´ˆ, {method_type})")
        
        log.info("\nâš¡ ì†ë„ ìˆœìœ„:")
        for i, (method, rouge_avg, time_taken) in enumerate(time_data, 1):
            method_type = experiment_results['methods'][method]['method_type']
            log.info(f"  {i}ìœ„. {method}: {time_taken:.1f}ì´ˆ (ROUGE-avg: {rouge_avg:.4f})")
        
        # ğŸ“Š ìƒì„¸ ì ìˆ˜ ì¶œë ¥
        log.info("\nğŸ“Š ìƒì„¸ ROUGE ì ìˆ˜:")
        for method_name, method_data in experiment_results['methods'].items():
            if 'rouge_scores' in method_data:
                scores = method_data['rouge_scores']
                log.info(f"\nğŸ”¹ {method_name}:")
                log.info(f"   ROUGE-1: {scores['rouge-1']:.4f}")
                log.info(f"   ROUGE-2: {scores['rouge-2']:.4f}")
                log.info(f"   ROUGE-L: {scores['rouge-l']:.4f}")
                log.info(f"   ROUGE-avg: {scores['rouge-avg']:.4f}")
                log.info(f"   ì‹¤í–‰ì‹œê°„: {method_data['time_seconds']:.1f}ì´ˆ")
        
        # ğŸ¯ ìµœì  ë°©ì‹ ì¶”ì²œ
        best_performance = performance_data[0]
        fastest_method = time_data[0]
        
        log.info("\n" + "ğŸ¯ " + "="*50)
        log.info("ğŸ’¡ ì¶”ì²œ ê²°ê³¼")
        log.info("="*50)
        log.info(f"ğŸ† ìµœê³  ì„±ëŠ¥: {best_performance[0]} (ROUGE-avg: {best_performance[1]:.4f})")
        log.info(f"âš¡ ìµœê³  ì†ë„: {fastest_method[0]} ({fastest_method[2]:.1f}ì´ˆ)")
        
        # ì„±ëŠ¥ vs ì†ë„ trade-off ë¶„ì„
        performance_gap = best_performance[1] - fastest_method[1] 
        speed_ratio = fastest_method[2] / best_performance[2]
        
        if performance_gap < 0.01 and speed_ratio < 0.5:
            log.info(f"ğŸ’ ì¶”ì²œ: {fastest_method[0]} (ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸í•˜ê³  ì†ë„ ìš°ìˆ˜)")
        elif performance_gap > 0.02:
            log.info(f"ğŸ’ ì¶”ì²œ: {best_performance[0]} (ì„±ëŠ¥ ì°¨ì´ ìœ ì˜ë¯¸)")
        else:
            log.info("ğŸ’­ ì„±ëŠ¥ê³¼ ì†ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ìš©ë„ì— ë§ê²Œ ì„ íƒí•˜ì„¸ìš”")
    
    # ê²°ê³¼ ì €ì¥
    results_dir = "./ensemble_results"
    os.makedirs(results_dir, exist_ok=True)
    
    experiment_metadata_path = os.path.join(results_dir, f"comprehensive_experiment_{timestamp}.json")
    with open(experiment_metadata_path, "w", encoding='utf-8') as f:
        json.dump(experiment_results, f, indent=2, ensure_ascii=False)
    log.info(f"\nğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {experiment_metadata_path}")
    
    log.info("\n" + "ğŸ‰ " + "="*50)
    log.info("âœ… ì¢…í•© ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
    log.info("="*50)
    
    return experiment_results

class PostProcessingEnsemble:
    """
    í›„ì²˜ë¦¬ ê¸°ë°˜ ì•™ìƒë¸” ì¶”ë¡  í´ë˜ìŠ¤
    ê° ëª¨ë¸ì´ ë…ë¦½ì ìœ¼ë¡œ ì™„ì „í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì•™ìƒë¸”:
    - í•˜ë“œ ë³´íŒ…: í† í° ë‹¨ìœ„ ë‹¤ìˆ˜ê²°
    - ì†Œí”„íŠ¸ ë³´íŒ…: í™•ë¥  ë¶„í¬ í‰ê· 
    - ê¸¸ì´ ê¸°ë°˜: ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
    """
    
    def __init__(self, model_paths, device="cuda:0"):
        """
        Args:
            model_paths: ëª¨ë¸ ZIP íŒŒì¼ ê²½ë¡œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.model_paths = model_paths
        self.device = device
        self.models = []
        self.tokenizers = []
        self.configs = []
        self.metadata_list = []
        
        log.info(f"ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {len(model_paths)}ê°œ ëª¨ë¸")
        log.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ë“¤ì„ ë¡œë”©"""
        log.info("ëª¨ë¸ë“¤ ë¡œë”© ì‹œì‘...")
        
        for i, path in enumerate(self.model_paths):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.model_paths)} ë¡œë”© ì¤‘: {path}")
            
            try:
                model, tokenizer, config, metadata = load_model_package(path)
                
                # GPU ë©”ëª¨ë¦¬ í™•ì¸ ë° ëª¨ë¸ ë¡œë”©
                try:
                    model.to(self.device)
                    model.eval()
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        log.error(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨. CPUë¡œ fallback ì‹œë„...")
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                        self.device = "cpu"
                        model.to(self.device)
                        model.eval()
                        log.warning(f"ëª¨ë¸ {i+1}ì„ CPUì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì„±ëŠ¥ì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        raise
                
                self.models.append(model)
                self.tokenizers.append(tokenizer)
                self.configs.append(config)
                self.metadata_list.append(metadata)
                
                log.info(f"ëª¨ë¸ {i+1} ë¡œë”© ì™„ë£Œ: {metadata.get('wandb_run_name', 'Unknown')} (device: {self.device})")
                
            except Exception as e:
                log.error(f"ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨: {e}")
                log.error(f"ê²½ë¡œ: {path}")
                raise
        
        log.info(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def generate_with_single_model(self, model, tokenizer, config, input_texts):
        """
        ë‹¨ì¼ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            model: ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €  
            config: ì„¤ì •
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for text in tqdm(input_texts, desc="í…ìŠ¤íŠ¸ ìƒì„± ì¤‘"):
            try:
                inputs = tokenizer(
                    text, 
                    return_tensors="pt", 
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                
                # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                for token in config['inference']['remove_tokens']:
                    generated_text = generated_text.replace(token, " ")
                
                results.append(generated_text.strip())
                
            except Exception as e:
                log.warning(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        return results
    
    def token_level_hard_voting(self, generated_texts_list, reference_tokenizer):
        """
        í† í° ë‹¨ìœ„ í•˜ë“œ ë³´íŒ…
        
        Args:
            generated_texts_list: ê° ëª¨ë¸ë³„ ìƒì„± í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë“¤
            reference_tokenizer: ê¸°ì¤€ í† í¬ë‚˜ì´ì €
            
        Returns:
            list: ì•™ìƒë¸” ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        ensemble_results = []
        num_samples = len(generated_texts_list[0])
        
        log.info("í† í° ë‹¨ìœ„ í•˜ë“œ ë³´íŒ… ì‹œì‘...")
        
        for i in tqdm(range(num_samples), desc="ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘"):
            # ê° ìƒ˜í”Œì— ëŒ€í•œ ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
            texts_for_sample = [texts[i] for texts in generated_texts_list]
            
            # ë¹ˆ ë¬¸ìì—´ ì œê±°
            texts_for_sample = [text for text in texts_for_sample if text.strip()]
            
            if not texts_for_sample:
                ensemble_results.append("")
                continue
            
            # í† í°í™”
            tokenized_texts = []
            for text in texts_for_sample:
                try:
                    tokens = reference_tokenizer.tokenize(text)
                    tokenized_texts.append(tokens)
                except:
                    # í† í°í™” ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    tokenized_texts.append([])
            
            # ë¹ˆ í† í° ë¦¬ìŠ¤íŠ¸ ì œê±°
            tokenized_texts = [tokens for tokens in tokenized_texts if tokens]
            
            if not tokenized_texts:
                ensemble_results.append("")
                continue
            
            # ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° ì •ë ¬
            max_len = max(len(tokens) for tokens in tokenized_texts)
            
            # ê° ìœ„ì¹˜ë³„ë¡œ ë‹¤ìˆ˜ê²°
            final_tokens = []
            for pos in range(max_len):
                tokens_at_pos = []
                for tokens in tokenized_texts:
                    if pos < len(tokens):
                        tokens_at_pos.append(tokens[pos])
                
                if tokens_at_pos:
                    # ê°€ì¥ ë§ì´ ì„ íƒëœ í† í°
                    token_counts = Counter(tokens_at_pos)
                    most_common_token = token_counts.most_common(1)[0][0]
                    final_tokens.append(most_common_token)
            
            # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            try:
                final_text = reference_tokenizer.convert_tokens_to_string(final_tokens)
                ensemble_results.append(final_text.strip())
            except:
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê°€ì¥ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                ensemble_results.append(texts_for_sample[0])
        
        log.info("í† í° ë‹¨ìœ„ í•˜ë“œ ë³´íŒ… ì™„ë£Œ")
        return ensemble_results
    
    def length_based_ensemble(self, input_texts, config):
        """
        ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”: ê° ëª¨ë¸ì˜ ê²°ê³¼ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì„ ì„ íƒ
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            
        Returns:
            list: ì•™ìƒë¸” ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        tokenizer = self.tokenizers[0]  # ê¸°ì¤€ í† í¬ë‚˜ì´ì €
        
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì‹œì‘...")
        
        for text in tqdm(input_texts, desc="ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘"):
            try:
                # ì…ë ¥ í† í¬ë‚˜ì´ì œì´ì…˜
                inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                # ê° ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì§ì ‘ ìƒì„±í•˜ì—¬ ê¸¸ì´ ê¸°ë°˜ ì„ íƒ
                model_results = []
                for model in self.models:
                    with torch.no_grad():
                        generated_ids = model.generate(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            max_length=config['inference']['generate_max_length'],
                            num_beams=config['inference']['num_beams'],
                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                            early_stopping=config['inference']['early_stopping']
                        )
                        
                        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                        
                        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                        for token in config['inference']['remove_tokens']:
                            generated_text = generated_text.replace(token, " ")
                        
                        model_results.append(generated_text.strip())
                
                # ê¸¸ì´ ê¸°ë°˜ ì„ íƒ: ê°€ì¥ ê¸´ ê²°ê³¼ë¥¼ ì„ íƒ
                if model_results:
                    # ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
                    longest_result = max(model_results, key=len)
                    results.append(longest_result)
                else:
                    results.append("")
                    
            except Exception as e:
                log.warning(f"ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì™„ë£Œ")
        return results
    
    def soft_voting_ensemble(self, input_texts, config):
        """
        ì§„ì§œ ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”: ëª¨ë¸ë“¤ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìƒì„±
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            
        Returns:
            list: ì•™ìƒë¸” ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        tokenizer = self.tokenizers[0]  # ê¸°ì¤€ í† í¬ë‚˜ì´ì €
        
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        
        for text in tqdm(input_texts, desc="ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘"):
            try:
                # ì…ë ¥ í† í¬ë‚˜ì´ì œì´ì…˜
                inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                # ê° ëª¨ë¸ì—ì„œ beam searchë¥¼ í†µí•´ ì—¬ëŸ¬ í›„ë³´ ìƒì„±
                model_candidates = []
                for model in self.models:
                    with torch.no_grad():
                        # beam searchë¡œ ì—¬ëŸ¬ í›„ë³´ ìƒì„±
                        outputs = model.generate(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            max_length=config['inference']['generate_max_length'],
                            num_beams=config['inference']['num_beams'],
                            num_return_sequences=min(3, config['inference']['num_beams']),  # ìµœëŒ€ 3ê°œ í›„ë³´
                            return_dict_in_generate=True,
                            output_scores=True,
                            early_stopping=config['inference']['early_stopping']
                        )
                        
                        # ê° í›„ë³´ì™€ ê·¸ ì ìˆ˜ë¥¼ ì €ì¥
                        candidates = []
                        for i, sequence in enumerate(outputs.sequences):
                            text_output = tokenizer.decode(sequence, skip_special_tokens=True)
                            
                            # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                            for token in config['inference']['remove_tokens']:
                                text_output = text_output.replace(token, " ")
                            
                            text_output = text_output.strip()
                            
                            # ì ìˆ˜ ê³„ì‚° (ê¸¸ì´ë¡œ ì •ê·œí™”ëœ í‰ê·  ì ìˆ˜)
                            if hasattr(outputs, 'sequences_scores') and len(outputs.sequences_scores) > i:
                                score = outputs.sequences_scores[i].item()
                            else:
                                # sequences_scoresê°€ ì—†ìœ¼ë©´ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ ì‚¬ìš©
                                score = len(text_output.split()) / config['inference']['generate_max_length']
                            
                            candidates.append((text_output, score))
                        
                        model_candidates.append(candidates)
                
                # ì†Œí”„íŠ¸ ë³´íŒ…: ê° ëª¨ë¸ì˜ ìµœê³  ì ìˆ˜ í›„ë³´ë“¤ ì¤‘ì—ì„œ í‰ê·  ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ê²ƒ ì„ íƒ
                all_candidates = []
                
                # ê° ëª¨ë¸ì˜ ëª¨ë“  í›„ë³´ë¥¼ ìˆ˜ì§‘
                for model_idx, candidates in enumerate(model_candidates):
                    for text_output, score in candidates:
                        all_candidates.append((text_output, score, model_idx))
                
                if all_candidates:
                    # ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì ìˆ˜ í‰ê·  ê³„ì‚°
                    text_scores = {}
                    text_counts = {}
                    
                    for text_output, score, model_idx in all_candidates:
                        if text_output not in text_scores:
                            text_scores[text_output] = 0
                            text_counts[text_output] = 0
                        text_scores[text_output] += score
                        text_counts[text_output] += 1
                    
                    # í‰ê·  ì ìˆ˜ ê³„ì‚°
                    for text_output in text_scores:
                        text_scores[text_output] /= text_counts[text_output]
                    
                    # ê°€ì¥ ë†’ì€ í‰ê·  ì ìˆ˜ë¥¼ ê°€ì§„ í…ìŠ¤íŠ¸ ì„ íƒ
                    best_text = max(text_scores.keys(), key=lambda x: text_scores[x])
                    results.append(best_text)
                else:
                    results.append("")
                    
            except Exception as e:
                log.warning(f"ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì™„ë£Œ")
        return results
    
    def evaluate_on_validation(self, val_data_path):
        """
        ê²€ì¦ ë°ì´í„°ë¡œ ì•™ìƒë¸” ë° ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            val_data_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            dict: í‰ê°€ ê²°ê³¼ (ê°œë³„ ëª¨ë¸ ì ìˆ˜, ì•™ìƒë¸” ì ìˆ˜)
        """
        log.info(f"ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘: {val_data_path}")
        
        # ê²€ì¦ ë°ì´í„° ë¡œë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©)
        val_df = pd.read_csv(val_data_path)
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²˜ìŒ 50ê°œë§Œ ì‚¬ìš©
        val_df = val_df.head(50)
        input_texts = val_df['dialogue'].tolist()
        reference_summaries = val_df['summary'].tolist()
        log.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£¼: {len(input_texts)}ê°œ ìƒ˜í”Œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)")
        
        # ê° ëª¨ë¸ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ìƒì„±
        all_generated_texts = []
        individual_scores = []
        
        for i, (model, tokenizer, config) in enumerate(zip(self.models, self.tokenizers, self.configs)):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.models)} ê²€ì¦ ì ìˆ˜ ê³„ì‚° ì‹œì‘ (baseline.py ë°©ì‹)...")
            
            # baseline.py ë°©ì‹ìœ¼ë¡œ ì •í™•í•œ ê²€ì¦ ì ìˆ˜ ê³„ì‚°
            rouge_scores = evaluate_single_model_with_baseline(model, tokenizer, config)
            individual_scores.append({
                'model_index': i + 1,
                'model_metadata': self.metadata_list[i],
                'rouge_scores': rouge_scores
            })
            
            log.info(f"ëª¨ë¸ {i+1} ê²€ì¦ ì ìˆ˜ (baseline.py ë°©ì‹) - ROUGE-1: {rouge_scores['rouge-1']:.6f}, "
                    f"ROUGE-2: {rouge_scores['rouge-2']:.6f}, ROUGE-L: {rouge_scores['rouge-l']:.6f}")
            
            # ì•™ìƒë¸”ìš© ì¶”ë¡  ë°ì´í„° ì¤€ë¹„ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
            generated_texts = self.generate_with_single_model(model, tokenizer, config, input_texts)
            all_generated_texts.append(generated_texts)
        
        # ì„¸ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ëª¨ë‘ í…ŒìŠ¤íŠ¸
        log.info("\n=== í•˜ë“œ ë³´íŒ… vs ì†Œí”„íŠ¸ ë³´íŒ… vs ê¸¸ì´ ê¸°ë°˜ ë¹„êµ ===")
        
        # 1. í•˜ë“œ ë³´íŒ… ì•™ìƒë¸”
        log.info("í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        hard_voting_results = self.token_level_hard_voting(all_generated_texts, self.tokenizers[0])
        
        # 2. ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        soft_voting_results = self.soft_voting_ensemble(input_texts, self.configs[0])
        
        # 3. ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì‹œì‘...")
        length_based_results = self.length_based_ensemble(input_texts, self.configs[0])
        
        # ROUGE ê³„ì‚° í•¨ìˆ˜ ì •ì˜
        def calculate_rouge_scores(predictions, references, method_name):
            from rouge import Rouge
            rouge = Rouge()
            
            # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
            cleaned_predictions = []
            cleaned_references = []
            for pred, ref in zip(predictions, references):
                pred_clean = pred.strip()
                ref_clean = ref.strip()
                for token in self.configs[0]['inference']['remove_tokens']:
                    pred_clean = pred_clean.replace(token, " ")
                    ref_clean = ref_clean.replace(token, " ")
                pred_clean = pred_clean.strip() if pred_clean.strip() else "empty"
                ref_clean = ref_clean.strip() if ref_clean.strip() else "empty"
                cleaned_predictions.append(pred_clean)
                cleaned_references.append(ref_clean)
            
            try:
                rouge_results = rouge.get_scores(cleaned_predictions, cleaned_references, avg=True)
                rouge_scores = {key: value["f"] for key, value in rouge_results.items()}
                # rouge-avg ê³„ì‚° ì¶”ê°€
                rouge_avg = (rouge_scores['rouge-1'] + rouge_scores['rouge-2'] + rouge_scores['rouge-l']) / 3
                rouge_scores['rouge-avg'] = rouge_avg
                
                log.info(f"{method_name} ê²€ì¦ ì ìˆ˜ - ROUGE-1: {rouge_scores['rouge-1']:.4f}, "
                        f"ROUGE-2: {rouge_scores['rouge-2']:.4f}, ROUGE-L: {rouge_scores['rouge-l']:.4f}, "
                        f"ROUGE-avg: {rouge_scores['rouge-avg']:.4f}")
                return rouge_scores
            except Exception as e:
                log.warning(f"{method_name} ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
                return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'rouge-avg': 0.0}
        
        # 3. ì„¸ ë°©ì‹ì˜ ROUGE ì ìˆ˜ ê³„ì‚°
        hard_voting_scores = calculate_rouge_scores(hard_voting_results, reference_summaries, "í•˜ë“œ ë³´íŒ…")
        soft_voting_scores = calculate_rouge_scores(soft_voting_results, reference_summaries, "ì†Œí”„íŠ¸ ë³´íŒ…")
        length_based_scores = calculate_rouge_scores(length_based_results, reference_summaries, "ê¸¸ì´ ê¸°ë°˜")
        
        # 4. ë¹„êµ ê²°ê³¼ ì¶œë ¥
        log.info("\n=== ì•™ìƒë¸” ë°©ì‹ ë¹„êµ ê²°ê³¼ ===")
        log.info(f"í•˜ë“œ ë³´íŒ… ROUGE-avg: {hard_voting_scores['rouge-avg']:.4f}")
        log.info(f"ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-avg: {soft_voting_scores['rouge-avg']:.4f}")
        log.info(f"ê¸¸ì´ ê¸°ë°˜ ROUGE-avg: {length_based_scores['rouge-avg']:.4f}")
        
        # ê°€ì¥ ë‚˜ì€ ë°©ì‹ ì„ íƒ
        all_scores = {
            "í•˜ë“œ ë³´íŒ…": (hard_voting_scores, hard_voting_results),
            "ì†Œí”„íŠ¸ ë³´íŒ…": (soft_voting_scores, soft_voting_results),
            "ê¸¸ì´ ê¸°ë°˜": (length_based_scores, length_based_results)
        }
        
        best_method = max(all_scores.keys(), key=lambda x: all_scores[x][0]['rouge-avg'])
        ensemble_rouge_scores = all_scores[best_method][0]
        
        log.info(f"{best_method}ì´ ê°€ì¥ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
        
        evaluation_results = {
            'individual_model_scores': individual_scores,
            'hard_voting_scores': hard_voting_scores,
            'soft_voting_scores': soft_voting_scores,
            'length_based_scores': length_based_scores,
            'ensemble_scores': ensemble_rouge_scores,
            'best_ensemble_method': best_method,
            'num_validation_samples': len(input_texts)
        }
        
        log.info("ê²€ì¦ ë°ì´í„° í‰ê°€ ì™„ë£Œ (baseline.py ë°©ì‹ ì‚¬ìš©)")
        return evaluation_results
    
    def run_ensemble(self, test_data_path):
        """
        í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ì‹¤í–‰
        
        Args:
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            tuple: (ensemble_result_df, individual_results_list)
        """
        log.info(f"ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘: {test_data_path}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©)
        test_df = pd.read_csv(test_data_path)
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²˜ìŒ 20ê°œë§Œ ì‚¬ìš©
        test_df = test_df.head(20)
        input_texts = test_df['dialogue'].tolist()
        log.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£¼: {len(input_texts)}ê°œ ìƒ˜í”Œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)")
        
        # ê°œë³„ ëª¨ë¸ë“¤ë¡œ ì¶”ë¡  ìˆ˜í–‰
        all_generated_texts = []
        
        for i, (model, tokenizer, config) in enumerate(zip(self.models, self.tokenizers, self.configs)):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.models)} ì¶”ë¡  ì‹œì‘...")
            log.info(f"ëª¨ë¸ ì„¤ì • - max_length: {config['inference']['generate_max_length']}, "
                    f"num_beams: {config['inference']['num_beams']}")
            
            generated_texts = self.generate_with_single_model(model, tokenizer, config, input_texts)
            all_generated_texts.append(generated_texts)
            
            log.info(f"ëª¨ë¸ {i+1} ì¶”ë¡  ì™„ë£Œ")
        
        # ì„¸ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ëª¨ë‘ ìˆ˜í–‰
        log.info("\n=== í•˜ë“œ ë³´íŒ… vs ì†Œí”„íŠ¸ ë³´íŒ… vs ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ìˆ˜í–‰ ===")
        
        # 1. í•˜ë“œ ë³´íŒ… ì•™ìƒë¸”
        log.info("í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        hard_voting_results = self.token_level_hard_voting(all_generated_texts, self.tokenizers[0])
        
        # 2. ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        soft_voting_results = self.soft_voting_ensemble(input_texts, self.configs[0])
        
        # 3. ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì‹œì‘...")
        length_based_results = self.length_based_ensemble(input_texts, self.configs[0])
        
        # 4. ì„¸ ë°©ì‹ì˜ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        hard_voting_df = pd.DataFrame({
            'fname': test_df['fname'],
            'summary': hard_voting_results
        })
        
        soft_voting_df = pd.DataFrame({
            'fname': test_df['fname'],
            'summary': soft_voting_results
        })
        
        length_based_df = pd.DataFrame({
            'fname': test_df['fname'],
            'summary': length_based_results
        })
        
        log.info("ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ (í•˜ë“œ ë³´íŒ… & ì†Œí”„íŠ¸ ë³´íŒ… & ê¸¸ì´ ê¸°ë°˜)")
        
        # ì„¸ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ëª¨ë‘ ë°˜í™˜
        ensemble_results = {
            'hard_voting': hard_voting_df,
            'soft_voting': soft_voting_df,
            'length_based': length_based_df,
            'individual_results': all_generated_texts
        }
        
        return ensemble_results, all_generated_texts

def run_single_method(method_name):
    """
    ê°œë³„ ì•™ìƒë¸” ë°©ì‹ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        method_name: ì‹¤í–‰í•  ë°©ì‹ ('hard_voting', 'soft_voting', 'length_based', 'realtime_token')
    """
    log.info(f"ğŸ¯ ê°œë³„ ë°©ì‹ ì‹¤í–‰: {method_name}")
    
    # ê³µí†µ í•¨ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    existing_model_paths = get_model_paths()
    if not existing_model_paths:
        return
    
    log.info(f"ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ë¡œ {method_name} ì§„í–‰")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    # ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ë°©ì‹
    if method_name == "realtime_token":
        ensemble = RealtimeTokenEnsemble(existing_model_paths, device=device)
        ensemble.load_models()
        
        # ê²€ì¦ ë°ì´í„° í‰ê°€
        val_data_path = "../../input/data/dev.csv"
        if os.path.exists(val_data_path):
            log.info("ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘")
            evaluation_results = ensemble.evaluate_on_validation(val_data_path)
            if evaluation_results:
                scores = evaluation_results['realtime_token_ensemble_scores']
                log.info(f"{method_name} ê²€ì¦ ì ìˆ˜ - ROUGE-avg: {scores['rouge-avg']:.4f}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ 
        test_data_path = "../../input/data/test.csv"
        if os.path.exists(test_data_path):
            ensemble_df, generation_time = ensemble.run_ensemble(test_data_path)
            
            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = "./ensemble_results"
            os.makedirs(results_dir, exist_ok=True)
            
            result_path = os.path.join(results_dir, f"{method_name}_{timestamp}.csv")
            ensemble_df.to_csv(result_path, index=False, encoding='utf-8')
            log.info(f"{method_name} ê²°ê³¼ ì €ì¥: {result_path}")
            log.info(f"{method_name} ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
    
    # í›„ì²˜ë¦¬ ë°©ì‹ë“¤ (hard_voting, soft_voting, length_based)
    else:
        ensemble = PostProcessingEnsemble(existing_model_paths, device=device)
        ensemble.load_models()
        
        # ê²€ì¦ ë°ì´í„°ë¡œ ê°œë³„ ë°©ì‹ í‰ê°€
        val_data_path = "../../input/data/dev.csv"
        if os.path.exists(val_data_path):
            log.info("ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘")
            val_df = pd.read_csv(val_data_path)
            val_df_sample = val_df.head(50)  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
            input_texts = val_df_sample['dialogue'].tolist()
            reference_summaries = val_df_sample['summary'].tolist()
            
            # ì„ íƒí•œ ë°©ì‹ìœ¼ë¡œë§Œ ìƒì„±
            if method_name == "hard_voting":
                # ëª¨ë“  ëª¨ë¸ë¡œ ìƒì„± í›„ í•˜ë“œ ë³´íŒ…
                generated_texts_list = []
                for model, tokenizer, config in zip(ensemble.models, ensemble.tokenizers, ensemble.configs):
                    texts = ensemble.generate_with_single_model(model, tokenizer, config, input_texts)
                    generated_texts_list.append(texts)
                results = ensemble.token_level_hard_voting(generated_texts_list, ensemble.tokenizers[0])
                
            elif method_name == "soft_voting":
                results = ensemble.soft_voting_ensemble(input_texts, ensemble.configs[0])
                
            elif method_name == "length_based":
                results = ensemble.length_based_ensemble(input_texts, ensemble.configs[0])
            
            # ROUGE ì ìˆ˜ ê³„ì‚°
            from rouge import Rouge
            rouge = Rouge()
            cleaned_predictions = []
            cleaned_references = []
            for pred, ref in zip(results, reference_summaries):
                pred_clean = pred.strip() if pred.strip() else "empty"
                ref_clean = ref.strip() if ref.strip() else "empty"
                cleaned_predictions.append(pred_clean)
                cleaned_references.append(ref_clean)
            
            try:
                rouge_results = rouge.get_scores(cleaned_predictions, cleaned_references, avg=True)
                rouge_scores = {key: value["f"] for key, value in rouge_results.items()}
                rouge_avg = (rouge_scores['rouge-1'] + rouge_scores['rouge-2'] + rouge_scores['rouge-l']) / 3
                rouge_scores['rouge-avg'] = rouge_avg
                log.info(f"{method_name} ê²€ì¦ ì ìˆ˜ - ROUGE-avg: {rouge_scores['rouge-avg']:.4f}")
            except Exception as e:
                log.warning(f"ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ 
        test_data_path = "../../input/data/test.csv"
        if os.path.exists(test_data_path):
            test_df = pd.read_csv(test_data_path)
            test_df_sample = test_df.head(20)  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
            test_input_texts = test_df_sample['dialogue'].tolist()
            
            # ì„ íƒí•œ ë°©ì‹ìœ¼ë¡œë§Œ ìƒì„±
            if method_name == "hard_voting":
                generated_texts_list = []
                for model, tokenizer, config in zip(ensemble.models, ensemble.tokenizers, ensemble.configs):
                    texts = ensemble.generate_with_single_model(model, tokenizer, config, test_input_texts)
                    generated_texts_list.append(texts)
                final_results = ensemble.token_level_hard_voting(generated_texts_list, ensemble.tokenizers[0])
                
            elif method_name == "soft_voting":
                final_results = ensemble.soft_voting_ensemble(test_input_texts, ensemble.configs[0])
                
            elif method_name == "length_based":
                final_results = ensemble.length_based_ensemble(test_input_texts, ensemble.configs[0])
            
            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = "./ensemble_results"
            os.makedirs(results_dir, exist_ok=True)
            
            result_df = pd.DataFrame({
                'fname': test_df_sample['fname'],
                'summary': final_results
            })
            
            result_path = os.path.join(results_dir, f"{method_name}_{timestamp}.csv")
            result_df.to_csv(result_path, index=False, encoding='utf-8')
            log.info(f"{method_name} ê²°ê³¼ ì €ì¥: {result_path}")
    
    log.info(f"ğŸ‰ {method_name} ì‹¤í–‰ ì™„ë£Œ!")

def main(ensemble_strategy="comprehensive"):
    """
    ì•™ìƒë¸” ì¶”ë¡  ë©”ì¸ í•¨ìˆ˜
    
    Args:
        ensemble_strategy: ì•™ìƒë¸” ì „ëµ ('comprehensive', 'hard_voting', 'soft_voting', 'length_based', 'realtime_token', 'post_token_voting', 'realtime_token_ensemble')
    """
    
    # ğŸ”¬ ì¢…í•© ì‹¤í—˜ ì‹¤í–‰ (ëª¨ë“  ë°©ì‹ ë¹„êµ)
    if ensemble_strategy == "comprehensive":
        return main_comprehensive_experiment()
    
    # ğŸ¯ ê°œë³„ ë°©ì‹ ì‹¤í–‰
    if ensemble_strategy in ["hard_voting", "soft_voting", "length_based", "realtime_token"]:
        return run_single_method(ensemble_strategy)
    
    # ê¸°ì¡´ ë‹¨ì¼ ì „ëµ ì‹¤í–‰ (í•˜ìœ„ í˜¸í™˜ì„±)
    log.info(f"ì„ íƒëœ ì•™ìƒë¸” ì „ëµ: {ensemble_strategy}")
    
    # ê³µí†µ í•¨ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    existing_model_paths = get_model_paths()
    if not existing_model_paths:
        return
    
    log.info(f"ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ë¡œ ì•™ìƒë¸” ì§„í–‰")
    
    # ì•™ìƒë¸” ê°ì²´ ìƒì„±
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    if ensemble_strategy == "realtime_token_ensemble":
        ensemble = RealtimeTokenEnsemble(existing_model_paths, device=device)
    else:  # post_token_voting (default)
        ensemble = PostProcessingEnsemble(existing_model_paths, device=device)
    
    # ëª¨ë¸ë“¤ ë¡œë”©
    try:
        ensemble.load_models()
    except Exception as e:
        log.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return
    
    # ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰
    val_data_path = "../../input/data/dev.csv"
    evaluation_results = None
    
    if os.path.exists(val_data_path):
        try:
            log.info("="*50)
            log.info("ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
            log.info("="*50)
            evaluation_results = ensemble.evaluate_on_validation(val_data_path)
            
            # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¡œê¹…
            log.info("ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
            for score_info in evaluation_results['individual_model_scores']:
                model_idx = score_info['model_index']
                scores = score_info['rouge_scores']
                model_name = score_info['model_metadata'].get('wandb_run_name', f'Model_{model_idx}')
                log.info(f"  {model_name}: ROUGE-avg={scores['rouge-avg']:.4f}")
            
            # ì•™ìƒë¸” ì„±ëŠ¥ ë¡œê¹…
            ensemble_scores = evaluation_results['ensemble_scores']
            log.info(f"ì•™ìƒë¸” ì„±ëŠ¥: ROUGE-avg={ensemble_scores['rouge-avg']:.4f}")
            
            # ê°œì„  ì •ë„ ê³„ì‚°
            best_individual_score = max([s['rouge_scores']['rouge-avg'] for s in evaluation_results['individual_model_scores']])
            improvement = ensemble_scores['rouge-avg'] - best_individual_score
            log.info(f"ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ : {improvement:+.4f}")
            
        except Exception as e:
            log.error(f"ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹¤íŒ¨: {e}")
    else:
        log.warning(f"ê²€ì¦ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ (ê²€ì¦ ì ìˆ˜ ê³„ì‚° ê±´ë„ˆëœ¨): {val_data_path}")
    
    # ì•™ìƒë¸” ì¶”ë¡  ì‹¤í–‰
    test_data_path = "../../input/data/test.csv"
    if not os.path.exists(test_data_path):
        log.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_data_path}")
        return
    
    try:
        ensemble_results, individual_results = ensemble.run_ensemble(test_data_path)
    except Exception as e:
        log.error(f"ì•™ìƒë¸” ì¶”ë¡  ì‹¤íŒ¨: {e}")
        return
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ensemble_results í´ë” ìƒì„±
    results_dir = "./ensemble_results"
    os.makedirs(results_dir, exist_ok=True)
    
    # í•˜ë“œ ë³´íŒ… ê²°ê³¼ ì €ì¥
    hard_voting_path = os.path.join(results_dir, f"ensemble_hard_voting_{timestamp}.csv")
    ensemble_results['hard_voting'].to_csv(hard_voting_path, index=False, encoding='utf-8')
    log.info(f"í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ê²°ê³¼ ì €ì¥: {hard_voting_path}")
    
    # ì†Œí”„íŠ¸ ë³´íŒ… ê²°ê³¼ ì €ì¥
    soft_voting_path = os.path.join(results_dir, f"ensemble_soft_voting_{timestamp}.csv")
    ensemble_results['soft_voting'].to_csv(soft_voting_path, index=False, encoding='utf-8')
    log.info(f"ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ê²°ê³¼ ì €ì¥: {soft_voting_path}")
    
    # ê¸¸ì´ ê¸°ë°˜ ê²°ê³¼ ì €ì¥
    length_based_path = os.path.join(results_dir, f"ensemble_length_based_{timestamp}.csv")
    ensemble_results['length_based'].to_csv(length_based_path, index=False, encoding='utf-8')
    log.info(f"ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ê²°ê³¼ ì €ì¥: {length_based_path}")
    
    # ê°œë³„ ëª¨ë¸ ê²°ê³¼ë“¤ ì €ì¥
    for i, individual_result in enumerate(individual_results):
        individual_df = pd.DataFrame({
            'fname': ensemble_results['hard_voting']['fname'],  # í•˜ë“œ ë³´íŒ… ê²°ê³¼ì˜ fname ì‚¬ìš©
            'summary': individual_result
        })
        individual_path = os.path.join(results_dir, f"individual_model_{i+1}_{timestamp}.csv")
        individual_df.to_csv(individual_path, index=False, encoding='utf-8')
        log.info(f"ê°œë³„ ëª¨ë¸ {i+1} ê²°ê³¼ ì €ì¥: {individual_path}")
    
    # ì•™ìƒë¸” ë©”íƒ€ë°ì´í„° ì €ì¥
    ensemble_metadata = {
        "timestamp": timestamp,
        "num_models": len(existing_model_paths),
        "model_paths": existing_model_paths,
        "device": device,
        "ensemble_strategies": ["hard_voting", "soft_voting", "length_based"],
        "model_metadata": ensemble.metadata_list,
        "evaluation_results": evaluation_results  # ê²€ì¦ ì ìˆ˜ ê²°ê³¼ ì¶”ê°€
    }
    
    metadata_path = os.path.join(results_dir, f"ensemble_comparison_metadata_{timestamp}.json")
    with open(metadata_path, "w", encoding='utf-8') as f:
        json.dump(ensemble_metadata, f, indent=2, ensure_ascii=False)
    log.info(f"ì•™ìƒë¸” ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    log.info("=" * 50)
    log.info(f"ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ! (í•˜ë“œ ë³´íŒ… & ì†Œí”„íŠ¸ ë³´íŒ… & ê¸¸ì´ ê¸°ë°˜)")
    log.info(f"ì‚¬ìš©ëœ ëª¨ë¸ ìˆ˜: {len(existing_model_paths)}")
    log.info(f"í•˜ë“œ ë³´íŒ… ê²°ê³¼: {hard_voting_path}")
    log.info(f"ì†Œí”„íŠ¸ ë³´íŒ… ê²°ê³¼: {soft_voting_path}")
    log.info(f"ê¸¸ì´ ê¸°ë°˜ ê²°ê³¼: {length_based_path}")
    
    # ê²€ì¦ ì ìˆ˜ ìš”ì•½ ì¶œë ¥
    if evaluation_results:
        log.info(f"í‰ê°€ ê²°ê³¼ ìš”ì•½ (í•˜ë“œ vs ì†Œí”„íŠ¸ vs ê¸¸ì´ ê¸°ë°˜ ë¹„êµ):")
        
        # í•˜ë“œ ë³´íŒ… ê²°ê³¼
        hard_scores = evaluation_results['hard_voting_scores']
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-1: {hard_scores['rouge-1']:.4f}")
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-2: {hard_scores['rouge-2']:.4f}")
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-L: {hard_scores['rouge-l']:.4f}")
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-avg: {hard_scores['rouge-avg']:.4f}")
        
        # ì†Œí”„íŠ¸ ë³´íŒ… ê²°ê³¼
        soft_scores = evaluation_results['soft_voting_scores']
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-1: {soft_scores['rouge-1']:.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-2: {soft_scores['rouge-2']:.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-L: {soft_scores['rouge-l']:.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-avg: {soft_scores['rouge-avg']:.4f}")
        
        # ê¸¸ì´ ê¸°ë°˜ ê²°ê³¼
        length_scores = evaluation_results['length_based_scores']
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-1: {length_scores['rouge-1']:.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-2: {length_scores['rouge-2']:.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-L: {length_scores['rouge-l']:.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-avg: {length_scores['rouge-avg']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ë°©ì‹
        best_method = evaluation_results.get('best_ensemble_method', 'Unknown')
        log.info(f"  ìµœê³  ì„±ëŠ¥ ë°©ì‹: {best_method}")
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        best_individual_score = max([s['rouge_scores']['rouge-avg'] for s in evaluation_results['individual_model_scores']])
        hard_improvement = hard_scores['rouge-avg'] - best_individual_score
        soft_improvement = soft_scores['rouge-avg'] - best_individual_score
        length_improvement = length_scores['rouge-avg'] - best_individual_score
        log.info(f"  í•˜ë“œ ë³´íŒ… ê°œì„ : {hard_improvement:+.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ê°œì„ : {soft_improvement:+.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ê°œì„ : {length_improvement:+.4f}")
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ ì •ë³´
        log.info("ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸:")
        for i, score_info in enumerate(evaluation_results['individual_model_scores']):
            scores = score_info['rouge_scores']
            model_name = score_info['model_metadata'].get('wandb_run_name', f'Model_{i+1}')
            log.info(f"    {model_name}: ROUGE-avg={scores['rouge-avg']:.4f}")
    
    log.info("=" * 50)
    
    return evaluation_results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ì•™ìƒë¸” ì¶”ë¡  ì‹œìŠ¤í…œ - ì—¬ëŸ¬ ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ì—¬ í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python ensemble_inference.py                     # ëª¨ë“  ë°©ì‹ ë¹„êµ ì‹¤í–‰
  python ensemble_inference.py --mode=all          # ëª¨ë“  ë°©ì‹ ë¹„êµ ì‹¤í–‰  
  python ensemble_inference.py --mode=hard_voting  # í•˜ë“œ ë³´íŒ…ë§Œ ì‹¤í–‰
  python ensemble_inference.py --mode=soft_voting  # ì†Œí”„íŠ¸ ë³´íŒ…ë§Œ ì‹¤í–‰
  python ensemble_inference.py --mode=length_based # ê¸¸ì´ ê¸°ë°˜ë§Œ ì‹¤í–‰
  python ensemble_inference.py --mode=realtime_token # ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”ë§Œ ì‹¤í–‰

ì•™ìƒë¸” ë°©ì‹ ì„¤ëª…:
  all           - 4ê°€ì§€ ë°©ì‹ì„ ëª¨ë‘ ë¹„êµí•˜ì—¬ ìµœì  ë°©ì‹ ì¶”ì²œ
  hard_voting   - ê° ëª¨ë¸ì´ ì™„ì „í•œ í…ìŠ¤íŠ¸ ìƒì„± í›„ í† í°ë³„ ë‹¤ìˆ˜ê²°
  soft_voting   - ê° ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìµœì  í›„ë³´ ì„ íƒ
  length_based  - ê° ëª¨ë¸ ê²°ê³¼ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì„ ì„ íƒ
  realtime_token- ë§¤ í† í°ë§ˆë‹¤ ëª¨ë“  ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìƒì„±
        """)
    
    parser.add_argument(
        '--mode', 
        type=str, 
        default='all',
        choices=['all', 'hard_voting', 'soft_voting', 'length_based', 'realtime_token'],
        help='ì‹¤í–‰í•  ì•™ìƒë¸” ë°©ì‹ ì„ íƒ (ê¸°ë³¸ê°’: all - ëª¨ë“  ë°©ì‹ ë¹„êµ)'
    )
    
    args = parser.parse_args()
    
    # ì„ íƒëœ ëª¨ë“œ ë¡œê¹…
    if args.mode == 'all':
        log.info("ğŸ”¬ ëª¨ë“  ì•™ìƒë¸” ë°©ì‹ ë¹„êµ ëª¨ë“œ ì‹œì‘")
        main("comprehensive")
    else:
        log.info(f"ğŸ¯ ê°œë³„ ë°©ì‹ ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
        main(args.mode)