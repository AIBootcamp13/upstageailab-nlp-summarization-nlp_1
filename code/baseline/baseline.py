# -*- coding: utf-8 -*-

# 스크립트 파일이 있는 디렉토리를 현재 작업 디렉토리로 설정
import os; os.chdir(os.path.dirname(os.path.abspath(__file__)))
import sys; sys.path.append('../utils')
import log_util as log

log.info("""baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aZolVv9C2NDPNO0Z0Ojq_VbkCfb75SnT

# **💁🏻🗨️💁🏻‍♂️대화 요약 Baseline code**
> **Dialogue Summarization** 경진대회에 오신 여러분 환영합니다! 🎉    
> 본 대회에서는 최소 2명에서 최대 7명이 등장하여 나누는 대화를 요약하는 BART 기반 모델의 baseline code를 제공합니다.     
> 주어진 데이터를 활용하여 일상 대화에 대한 요약을 효과적으로 생성하는 모델을 만들어봅시다!

## ⚙️ 데이터 및 환경설정

### 1) 필요한 라이브러리 설치

- 필요한 라이브러리를 설치한 후 불러옵니다.
""")

import pandas as pd
import re
import json
import yaml
from tqdm import tqdm
from pprint import pprint
import random
import numpy as np
import shutil
import zipfile
import time
from datetime import datetime

import torch
import pytorch_lightning as pl
from rouge import Rouge # 모델의 성능을 평가하기 위한 라이브러리입니다.
from torch.utils.data import Dataset , DataLoader
from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback, TrainerCallback
import wandb # 모델 학습 과정을 손쉽게 Tracking하고, 시각화할 수 있는 라이브러리입니다.

class LoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and 'eval_loss' in logs:
            log.info(f"Evaluation metrics: {logs}")

from dotenv import load_dotenv
load_dotenv() # .env 파일 로드

def set_seed_for_reproducibility(seed=42):
    """
    완전한 재현성을 위한 시드 설정 함수
    
    Args:
        seed (int): 설정할 시드 값
    """
    log.info(f"재현성을 위한 시드 설정: {seed}")
    
    # Python random 시드 설정
    random.seed(seed)
    
    # NumPy random 시드 설정  
    np.random.seed(seed)
    
    # PyTorch 시드 설정
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # multi-GPU인 경우
    
    # CUDA deterministic 모드 활성화
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # PyTorch Lightning 시드 설정
    pl.seed_everything(seed, workers=True)
    
    # 환경 변수 설정
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    # CUBLAS 환경 변수 설정 - CUDA 10.2 이상에서 필요
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    # PyTorch 결정론적 알고리즘 활성화
    torch.use_deterministic_algorithms(True)
    
    log.info("CUBLAS_WORKSPACE_CONFIG 추가 설정 완료")
    log.info("torch.use_deterministic_algorithms(True) 설정 완료")
    log.info("모든 라이브러리에 대한 시드 설정이 완료되었습니다.")

def setup_wandb_login():
    """
    wandb 로그인 처리 함수
    
    Returns:
        bool: 로그인 성공 여부
    """
    # 이미 wandb가 초기화되어 있으면 로그인 시도하지 않음
    if wandb.run is not None:
        log.info("wandb가 이미 초기화되어 있습니다. 로그인을 건너뜁니다.")
        return True
        
    api_key = os.getenv('WANDB_API_KEY')
    if not api_key or api_key == 'your_wandb_api_key_here':
        log.info("WANDB_API_KEY가 설정되지 않았습니다. wandb 로그인을 건너뜁니다.")
        return False
    
    try:
        wandb.login(key=api_key)
        log.info("wandb 로그인 성공")
        return True
    except Exception as e:
        log.error(f"wandb 로그인 실패: {e}")
        return False


def create_default_config():
    """기본 설정을 생성하는 함수"""
    # config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의해줍니다.
    tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")
    
    config_data = {
        "general": {
            "data_path": "../../input/data/", # 모델 생성에 필요한 데이터 경로를 사용자 환경에 맞게 지정합니다.
            "model_name": "digit82/kobart-summarization", # 불러올 모델의 이름을 사용자 환경에 맞게 지정할 수 있습니다.
            "output_dir": "./" # 모델의 최종 출력 값을 저장할 경로를 설정합니다.
        },
        "tokenizer": {
            "encoder_max_len": 512,
            "decoder_max_len": 100,
            "bos_token": f"{tokenizer.bos_token}",
            "eos_token": f"{tokenizer.eos_token}",
            # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.
            "special_tokens": ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']
        },
        "training": {
            "overwrite_output_dir": True,
            "num_train_epochs": 20,
            "learning_rate": 1e-5,
            "per_device_train_batch_size": 50,
            "per_device_eval_batch_size": 32,
            "warmup_ratio": 0.1,
            "weight_decay": 0.01,
            "lr_scheduler_type": 'cosine',
            "optim": 'adamw_torch',
            "gradient_accumulation_steps": 1,
            "evaluation_strategy": 'epoch',
            "save_strategy": 'epoch',
            "save_total_limit": 5,
            "fp16": True,
            "load_best_model_at_end": True,
            "seed": 42,
            "logging_dir": "./logs",
            "logging_strategy": "epoch",
            "predict_with_generate": True,
            "generation_max_length": 100,
            "do_train": True,
            "do_eval": True,
            "early_stopping_patience": 3,
            "early_stopping_threshold": 0.001,
            "report_to": "wandb" # (선택) wandb를 사용할 때 설정합니다.
        },
        # (선택) wandb 홈페이지에 가입하여 얻은 정보를 기반으로 작성합니다.
        "wandb": {
            "entity": os.getenv("WANDB_ENTITY", "wandb_repo"),
            "project": os.getenv("WANDB_PROJECT", "project_name"),
            "name": os.getenv("WANDB_RUN_NAME", "run_name")
        },
        "inference": {
            "ckt_path": "model ckt path", # 사전 학습이 진행된 모델의 checkpoint를 저장할 경로를 설정합니다.
            "result_path": "./prediction/",
            "no_repeat_ngram_size": 2,
            "early_stopping": True,
            "generate_max_length": 100,
            "num_beams": 4,
            "batch_size" : 32,
            # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의합니다.
            "remove_tokens": ['<usr>', f"{tokenizer.bos_token}", f"{tokenizer.eos_token}", f"{tokenizer.pad_token}"]
        }
    }
    return config_data

def save_config(config_data, config_path="./config.yaml"):
    """설정을 YAML 파일로 저장하는 함수"""
    if not os.path.exists(config_path):
        with open(config_path, "w") as file:
            yaml.dump(config_data, file, allow_unicode=True)
        log.info(f"새로운 config.yaml 파일을 생성했습니다: {config_path}")
    else:
        log.info(f"기존 config.yaml 파일을 사용합니다: {config_path}")


def load_config(config_path="./config.yaml"):
    """설정 파일을 로드하고 처리하는 함수"""
    log.info("""- 참고✅    
: wandb 라이브러리를 사용하기 위해선 entity, project, name를 지정해주어야 합니다. wandb 홈페이지에 가입한 후 얻은 정보를 입력하여 작동할 수 있습니다.
""")
    
    # Config 파일이 없으면 생성
    if not os.path.exists(config_path):
        config_data = create_default_config()
        save_config(config_data, config_path)
    
    log.info("""### 3) Configuration 불러오기""")
    
    # 저장된 config 파일을 불러옵니다.
    with open(config_path, "r") as file:
        loaded_config = yaml.safe_load(file)
    
    # 불러온 config 파일의 전체 내용을 확인합니다.
    log.info(loaded_config)
    
    # wandb 훈련 추적 사용 여부 확인하고 비활성화
    use_train_tracking = os.getenv('WANDB_TRAIN_TRACKING', '').lower() == 'true'
    if not use_train_tracking:
        # wandb를 완전히 비활성화
        os.environ['WANDB_DISABLED'] = 'true'
    
    # (선택) 환경변수에서 wandb config 설정 업데이트
    if loaded_config['training']['report_to'] == 'wandb' and use_train_tracking:
        loaded_config['wandb']['entity'] = os.getenv('WANDB_ENTITY', loaded_config['wandb']['entity'])
        loaded_config['wandb']['name'] = os.getenv('WANDB_RUN_NAME', loaded_config['wandb']['name'])
        loaded_config['wandb']['project'] = os.getenv('WANDB_PROJECT', loaded_config['wandb']['project'])
    else:
        # wandb를 사용하지 않으므로 report_to를 None으로 변경
        loaded_config['training']['report_to'] = None
    
    return loaded_config

def load_and_preview_data(config):
    """데이터를 로드하고 미리보기하는 함수"""
    log.info("""### 4) 데이터 불러와서 확인해보기
- 실험에서 쓰일 데이터를 load하여 데이터의 구조와 내용을 살펴보겠습니다.
- Train, dev, test 순서대로 12457, 499, 250개 씩 데이터가 구성되어 있습니다.
""")
    
    # config에 저장된 데이터 경로를 통해 train과 validation data를 불러옵니다.
    data_path = config['general']['data_path']
    
    # train data의 구조와 내용을 확인합니다.
    train_df = pd.read_csv(os.path.join(data_path,'train.csv'))
    log.info("Train data tail:")
    log.info(train_df.tail())
    
    # validation data의 구조와 내용을 확인합니다.
    val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))
    log.info("Validation data tail:")
    log.info(val_df.tail())
    
    return train_df, val_df

log.info("""## 1. 데이터 가공 및 데이터셋 클래스 구축
- csv file 을 불러와서 encoder 와 decoder의 입력형태로 가공해줍니다.
- 가공된 데이터를 torch dataset class 로 구축하여 모델에 입력가능한 형태로 만듭니다.
""")

# 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성합니다.
class Preprocess:
    def __init__(self,
            bos_token: str,
            eos_token: str,
        ) -> None:

        self.bos_token = bos_token
        self.eos_token = eos_token

    @staticmethod
    # 실험에 필요한 컬럼을 가져옵니다.
    def make_set_as_df(file_path, is_train = True):
        if is_train:
            df = pd.read_csv(file_path)
            train_df = df[['fname','dialogue','summary']]
            return train_df
        else:
            df = pd.read_csv(file_path)
            test_df = df[['fname','dialogue']]
            return test_df

    # BART 모델의 입력, 출력 형태를 맞추기 위해 전처리를 진행합니다.
    def make_input(self, dataset,is_test = False):
        if is_test:
            encoder_input = dataset['dialogue']
            decoder_input = [self.bos_token] * len(dataset['dialogue'])
            return encoder_input.tolist(), list(decoder_input)
        else:
            encoder_input = dataset['dialogue']
            decoder_input = dataset['summary'].apply(lambda x : self.bos_token + str(x)) # Ground truth를 디코더의 input으로 사용하여 학습합니다.
            decoder_output = dataset['summary'].apply(lambda x : str(x) + self.eos_token)
            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()

# Train에 사용되는 Dataset 클래스를 정의합니다.
class DatasetForTrain(Dataset):
    def __init__(self, encoder_input, decoder_input, labels, len):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.len = len

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]
        item2['decoder_input_ids'] = item2['input_ids']
        item2['decoder_attention_mask'] = item2['attention_mask']
        item2.pop('input_ids')
        item2.pop('attention_mask')
        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]
        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]
        return item

    def __len__(self):
        return self.len

# Validation에 사용되는 Dataset 클래스를 정의합니다.
class DatasetForVal(Dataset):
    def __init__(self, encoder_input, decoder_input, labels, len):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.len = len

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]
        item2['decoder_input_ids'] = item2['input_ids']
        item2['decoder_attention_mask'] = item2['attention_mask']
        item2.pop('input_ids')
        item2.pop('attention_mask')
        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]
        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]
        return item

    def __len__(self):
        return self.len

# Test에 사용되는 Dataset 클래스를 정의합니다.
class DatasetForInference(Dataset):
    def __init__(self, encoder_input, test_id, len):
        self.encoder_input = encoder_input
        self.test_id = test_id
        self.len = len

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item['ID'] = self.test_id[idx]
        return item

    def __len__(self):
        return self.len

# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.
def prepare_train_dataset(config, preprocessor, data_path, tokenizer):
    train_file_path = os.path.join(data_path,'train.csv')
    val_file_path = os.path.join(data_path,'dev.csv')

    # train, validation에 대해 각각 데이터프레임을 구축합니다.
    train_data = preprocessor.make_set_as_df(train_file_path)
    val_data = preprocessor.make_set_as_df(val_file_path)

    log.info('-'*150)
    log.info(f'train_data:\n {train_data["dialogue"][0]}')
    log.info(f'train_label:\n {train_data["summary"][0]}')

    log.info('-'*150)
    log.info(f'val_data:\n {val_data["dialogue"][0]}')
    log.info(f'val_label:\n {val_data["summary"][0]}')

    encoder_input_train , decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)
    encoder_input_val , decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)
    log.info('-'*10, 'Load data complete', '-'*10,)

    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors="pt", padding=True,
                            add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)
    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors="pt", padding=True,
                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)
    tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors="pt", padding=True,
                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)

    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs,len(encoder_input_train))

    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors="pt", padding=True,
                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)
    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors="pt", padding=True,
                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)
    val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors="pt", padding=True,
                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)

    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs,len(encoder_input_val))

    log.info('-'*10, 'Make dataset complete', '-'*10,)
    return train_inputs_dataset, val_inputs_dataset

log.info("""## 2. Trainer 및 Trainingargs 구축하기
- Huggingface 의 Trainer 와 Training arguments를 활용하여 모델 학습을 일괄적으로 처리해주는 클래스를 정의합니다.
""")

# 모델 성능에 대한 평가 지표를 정의합니다. 본 대회에서는 ROUGE 점수를 통해 모델의 성능을 평가합니다.
def compute_metrics(config,tokenizer,pred):
    rouge = Rouge()
    predictions = pred.predictions
    labels = pred.label_ids

    predictions[predictions == -100] = tokenizer.pad_token_id
    labels[labels == -100] = tokenizer.pad_token_id

    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)
    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)

    # 정확한 평가를 위해 미리 정의된 불필요한 생성토큰들을 제거합니다.
    replaced_predictions = decoded_preds.copy()
    replaced_labels = labels.copy()
    remove_tokens = config['inference']['remove_tokens']
    for token in remove_tokens:
        replaced_predictions = [sentence.replace(token," ") for sentence in replaced_predictions]
        replaced_labels = [sentence.replace(token," ") for sentence in replaced_labels]

    log.info('-'*150)
    log.info(f"PRED: {replaced_predictions[0]}")
    log.info(f"GOLD: {replaced_labels[0]}")
    log.info('-'*150)
    log.info(f"PRED: {replaced_predictions[1]}")
    log.info(f"GOLD: {replaced_labels[1]}")
    log.info('-'*150)
    log.info(f"PRED: {replaced_predictions[2]}")
    log.info(f"GOLD: {replaced_labels[2]}")

    # 최종적인 ROUGE 점수를 계산합니다.
    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)
    
    # ROUGE 점수 결과를 로그에 출력
    log.info('-'*150)
    log.info("ROUGE Evaluation Results:")
    for metric_name, metric_values in results.items():
        log.info(f"{metric_name.upper()}: Precision={metric_values['p']:.4f}, Recall={metric_values['r']:.4f}, F1={metric_values['f']:.4f}")
    
    log.info('-'*150)

    # ROUGE 점수 중 F-1 score를 통해 평가합니다.
    result = {key: value["f"] for key, value in results.items()}
    return result

# 학습을 위한 trainer 클래스와 매개변수를 정의합니다.
def load_trainer_for_train(config,generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset):
    log.info('-'*10, 'Make training arguments', '-'*10,)
    
    # set training args
    training_args = Seq2SeqTrainingArguments(
                output_dir=config['general']['output_dir'], # model output directory
                overwrite_output_dir=config['training']['overwrite_output_dir'],
                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs
                learning_rate=config['training']['learning_rate'], # learning_rate
                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training
                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation
                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler
                weight_decay=config['training']['weight_decay'],  # strength of weight decay
                lr_scheduler_type=config['training']['lr_scheduler_type'],
                optim =config['training']['optim'],
                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
                eval_strategy=config['training']['evaluation_strategy'], # evaluation strategy to adopt during training
                save_strategy =config['training']['save_strategy'],
                save_total_limit=config['training']['save_total_limit'], # number of total save model.
                fp16=config['training']['fp16'],
                load_best_model_at_end=config['training']['load_best_model_at_end'], # 최종적으로 가장 높은 점수 저장
                seed=config['training']['seed'],
                logging_dir=config['training']['logging_dir'], # directory for storing logs
                logging_strategy=config['training']['logging_strategy'],
                predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score
                generation_max_length=config['training']['generation_max_length'],
                do_train=config['training']['do_train'],
                do_eval=config['training']['do_eval'],
                report_to=config['training']['report_to'] # (선택) wandb를 사용할 때 설정합니다.
            )

    # wandb 사용 시 모델 checkpoint 아티팩트 업로드 비활성화 (스토리지 절약)
    if config['training']['report_to'] == 'wandb':
        os.environ["WANDB_LOG_MODEL"]="false"
        os.environ["WANDB_WATCH"]="false"

    # Validation loss가 더 이상 개선되지 않을 때 학습을 중단시키는 EarlyStopping 기능을 사용합니다.
    MyCallback = EarlyStoppingCallback(
        early_stopping_patience=config['training']['early_stopping_patience'],
        early_stopping_threshold=config['training']['early_stopping_threshold']
    )
    log.info('-'*10, 'Make training arguments complete', '-'*10,)
    log.info('-'*10, 'Make trainer', '-'*10,)

    # Trainer 클래스를 정의합니다.
    trainer = Seq2SeqTrainer(
        model=generate_model, # 사용자가 사전 학습하기 위해 사용할 모델을 입력합니다.
        args=training_args,
        train_dataset=train_inputs_dataset,
        eval_dataset=val_inputs_dataset,
        compute_metrics = lambda pred: compute_metrics(config,tokenizer, pred),
        callbacks = [MyCallback, LoggingCallback()]
    )
    log.info('-'*10, 'Make trainer complete', '-'*10,)

    return trainer

# 학습을 위한 tokenizer와 사전 학습된 모델을 불러옵니다.
def load_tokenizer_and_model_for_train(config,device):
    log.info('-'*10, 'Load tokenizer & model', '-'*10,)
    log.info('-'*10, f'Model Name : {config["general"]["model_name"]}', '-'*10,)
    model_name = config['general']['model_name']
    bart_config = BartConfig().from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    generate_model = BartForConditionalGeneration.from_pretrained(config['general']['model_name'],config=bart_config)

    special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}
    tokenizer.add_special_tokens(special_tokens_dict)

    generate_model.resize_token_embeddings(len(tokenizer)) # 사전에 special token을 추가했으므로 재구성 해줍니다.
    generate_model.to(device)
    log.info(generate_model.config)

    log.info('-'*10, 'Load tokenizer & model complete', '-'*10,)
    return generate_model , tokenizer

def save_model_package(model, tokenizer, config, save_dir="models"):
    """
    모델, 토크나이저, 설정을 ZIP으로 패키징하여 저장
    
    Args:
        model: 학습된 모델
        tokenizer: 토크나이저
        config: 설정 딕셔너리
        save_dir: 저장할 디렉토리
    
    Returns:
        tuple: (zip_path, metadata)
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"model_baseline_{timestamp}.zip"
    
    # 임시 폴더 생성
    temp_dir = f"temp_{timestamp}"
    os.makedirs(temp_dir, exist_ok=True)
    
    try:
        log.info(f"모델 패키징 시작: {filename}")
        
        # 1. 모델 저장 (safetensors 형식)
        model.save_pretrained(temp_dir, safe_serialization=True)
        log.info("모델 저장 완료")
        
        # 2. 토크나이저 저장
        tokenizer_dir = os.path.join(temp_dir, "tokenizer")
        tokenizer.save_pretrained(tokenizer_dir)
        log.info("토크나이저 저장 완료")
        
        # 3. 설정 파일 저장
        log.info("저장할 config 내용:")
        log.info(f"  generation_max_length: {config['training']['generation_max_length']}")
        log.info(f"  encoder_max_len: {config['tokenizer']['encoder_max_len']}")
        log.info(f"  decoder_max_len: {config['tokenizer']['decoder_max_len']}")
        log.info(f"  learning_rate: {config['training']['learning_rate']}")
        log.info(f"  seed: {config['training']['seed']}")
        
        with open(os.path.join(temp_dir, "config.yaml"), "w", encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True)
        log.info("설정 파일 저장 완료")
            
        # 4. 메타데이터 저장
        metadata = {
            "timestamp": timestamp,
            "model_info": {
                "model_name": config['general']['model_name'],
                "vocab_size": len(tokenizer)
            }
        }
        
        # WandB 정보 추가 (있으면)
        try:
            if wandb.run is not None:
                metadata.update({
                    "wandb_run_id": wandb.run.id,
                    "wandb_run_name": wandb.run.name,
                    "wandb_run_url": wandb.run.url
                })
                log.info(f"WandB 정보 추가: {wandb.run.id}")
        except NameError:
            # wandb가 임포트되지 않은 경우 무시
            pass
            
        with open(os.path.join(temp_dir, "metadata.json"), "w", encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        log.info("메타데이터 저장 완료")
        
        # 5. ZIP 압축
        os.makedirs(save_dir, exist_ok=True)
        zip_path = os.path.join(save_dir, filename)
        shutil.make_archive(zip_path[:-4], 'zip', temp_dir)
        log.info(f"ZIP 압축 완료: {zip_path}")
        
        return zip_path, metadata
        
    except Exception as e:
        log.error(f"모델 패키징 중 오류 발생: {e}")
        raise
        
    finally:
        # 임시 폴더 삭제
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)
            log.info("임시 폴더 정리 완료")

log.info("""## 3. 모델 학습하기

- 앞에서 구축한 클래스 및 함수를 활용하여 학습 진행합니다.
""")

def train_model(config):
    """모델 학습을 수행하는 함수"""
    # 사용할 device를 정의합니다.
    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')
    log.info('-'*10, f'device : {device}', '-'*10,)
    log.info(torch.__version__)

    # 사용할 모델과 tokenizer를 불러옵니다.
    generate_model , tokenizer = load_tokenizer_and_model_for_train(config,device)
    log.info('-'*10,"tokenizer special tokens : ",tokenizer.special_tokens_map,'-'*10)

    # 학습에 사용할 데이터셋을 불러옵니다.
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token']) # decoder_start_token: str, eos_token: str
    data_path = config['general']['data_path']
    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config,preprocessor, data_path, tokenizer)

    # Trainer 클래스를 불러옵니다.
    trainer = load_trainer_for_train(config, generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset)
    trainer.train()   # 모델 학습을 시작합니다.

    # 학습 완료 후 모델을 ZIP으로 패키징하여 저장
    try:
        zip_path, metadata = save_model_package(trainer.model, tokenizer, config)
        log.info(f"모델이 ZIP 파일로 저장되었습니다: {zip_path}")
        log.info(f"메타데이터: {metadata}")
    except Exception as e:
        log.warning(f"모델 패키징 중 오류 발생하지만 학습은 완료됨: {e}")
    
    # 학습된 최상의 모델과 토크나이저 반환
    return trainer.model, tokenizer


# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.
def prepare_test_dataset(config,preprocessor, tokenizer):
    log.info("""- test data를 사용하여 모델의 성능을 확인합니다.""")

    test_file_path = os.path.join(config['general']['data_path'],'test.csv')

    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)
    test_id = test_data['fname']

    log.info('-'*150)
    log.info(f'test_data:\n{test_data["dialogue"][0]}')
    log.info('-'*150)

    encoder_input_test , decoder_input_test = preprocessor.make_input(test_data,is_test=True)
    log.info('-'*10, 'Load data complete', '-'*10,)

    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors="pt", padding=True,
                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,)
    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors="pt", padding=True,
                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,)

    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))
    log.info('-'*10, 'Make dataset complete', '-'*10,)

    return test_data, test_encoder_inputs_dataset

# 추론을 위한 tokenizer와 학습시킨 모델을 불러옵니다.
def load_tokenizer_and_model_for_test(config,device):
    log.info('-'*10, 'Load tokenizer & model', '-'*10,)

    model_name = config['general']['model_name']
    ckt_path = config['inference']['ckt_path']
    log.info('-'*10, f'Model Name : {model_name}', '-'*10,)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}
    tokenizer.add_special_tokens(special_tokens_dict)

    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)
    generate_model.resize_token_embeddings(len(tokenizer))
    generate_model.to(device)
    log.info('-'*10, 'Load tokenizer & model complete', '-'*10,)

    return generate_model , tokenizer

# 학습된 모델이 생성한 요약문의 출력 결과를 보여줍니다.
def inference(config, model=None, tokenizer=None):
    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')
    log.info('-'*10, f'device : {device}', '-'*10,)
    log.info(torch.__version__)

    # 모델과 토크나이저가 전달되면 사용, 아니면 기존 로직 사용 (하위 호환성)
    if model is not None and tokenizer is not None:
        generate_model = model
        log.info('-'*10, 'Using provided model and tokenizer', '-'*10,)
    else:
        generate_model , tokenizer = load_tokenizer_and_model_for_test(config,device)

    data_path = config['general']['data_path']
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])

    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config,preprocessor, tokenizer)
    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])

    summary = []
    text_ids = []
    with torch.no_grad():
        for item in tqdm(dataloader):
            text_ids.extend(item['ID'])
            generated_ids = generate_model.generate(input_ids=item['input_ids'].to('cuda:0'),
                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                            early_stopping=config['inference']['early_stopping'],
                            max_length=config['inference']['generate_max_length'],
                            num_beams=config['inference']['num_beams'],
                        )
            for ids in generated_ids:
                result = tokenizer.decode(ids)
                summary.append(result)

    # 정확한 평가를 위하여 노이즈에 해당되는 스페셜 토큰을 제거합니다.
    remove_tokens = config['inference']['remove_tokens']
    preprocessed_summary = summary.copy()
    for token in remove_tokens:
        preprocessed_summary = [sentence.replace(token," ") for sentence in preprocessed_summary]

    output = pd.DataFrame(
        {
            "fname": test_data['fname'],
            "summary" : preprocessed_summary,
        }
    )
    result_path = config['inference']['result_path']
    if not os.path.exists(result_path):
        os.makedirs(result_path)
    output_path = os.path.join(result_path, "output.csv")
    output.to_csv(output_path, index=False)
    log.info(f"추론 결과를 {output_path}에 저장했습니다.")

    # WandB 아티팩트 업로드 (조건부)
    try:
        if wandb.run is not None:
            artifact = wandb.Artifact(
                name="inference_results",
                type="predictions", 
                description="Test dataset inference results"
            )
            artifact.add_file(output_path)
            wandb.log_artifact(artifact)
            log.info(f"추론 결과를 WandB 아티팩트로 업로드했습니다: {output_path}")
    except NameError:
        # wandb가 임포트되지 않은 경우 무시
        pass
    except Exception as e:
        log.warning(f"WandB 아티팩트 업로드 중 오류 발생: {e}")

    return output

def run_inference(config, model=None, tokenizer=None):
    """추론을 실행하는 함수"""
    output = inference(config, model=model, tokenizer=tokenizer)
    log.info(output)  # 각 대화문에 대한 요약문이 출력됨을 확인할 수 있습니다.
    return output


def main():
    """전체 파이프라인을 실행하는 메인 함수"""
    # Config 로드
    config = load_config()
    
    # 재현성을 위한 시드 설정 (config에서 시드 값 가져오기)
    seed = config['training']['seed']
    set_seed_for_reproducibility(seed)
    
    # WandB 세션 관리
    use_train_tracking = os.getenv('WANDB_TRAIN_TRACKING', '').lower() == 'true'
    wandb_initialized = False
    
    try:
        # 데이터 미리보기
        load_and_preview_data(config)
        
        # WandB 초기화 (필요한 경우)
        if config['training']['report_to'] == 'wandb' and use_train_tracking:
            if setup_wandb_login():
                wandb.init(
                    entity=config['wandb']['entity'],
                    project=config['wandb']['project'],
                    name=config['wandb']['name'],
                )
                wandb_initialized = True
                log.info("WandB 세션이 초기화되었습니다.")
            else:
                log.warning("wandb 로그인 실패로 인해 wandb tracking이 비활성화됩니다.")
                config['training']['report_to'] = None
        
        # 학습 실행 및 최상의 모델 획득
        model, tokenizer = train_model(config)
        
        # 추론 실행 (학습된 최상의 모델 사용)
        output = run_inference(config, model=model, tokenizer=tokenizer)
        
        return output
        
    finally:
        # WandB 세션 안전하게 종료
        if wandb_initialized:
            try:
                wandb.finish()
                log.info("WandB 세션이 종료되었습니다.")
            except Exception as e:
                log.warning(f"WandB 세션 종료 중 오류 발생: {e}")


if __name__ == "__main__":
    main()

