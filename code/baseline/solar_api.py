# -*- coding: utf-8 -*-

# 스크립트 파일이 있는 디렉토리를 현재 작업 디렉토리로 설정
import os; os.chdir(os.path.dirname(os.path.abspath(__file__)))
import sys; sys.path.append('../utils')
import log_util as log

log.info("""solar_api.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZDwSFj5xh18hfTOmIocChNSwY_Scq1gj

# **💁🏻🗨️💁🏻‍♂️대화 요약 SOLAR API code**
> **Dialogue Summarization** 경진대회에 오신 여러분 환영합니다! 🎉    
> 본 자료에서는 Solar Chat API를 이용하여 대화 요약 대회를 풀어봅니다.

## ⚙️ 데이터 및 환경설정

### 1) 필요한 라이브러리 설치

- 필요한 라이브러리를 설치한 후 불러옵니다.
""")

# !pip install openai

import pandas as pd
import os
import time
from tqdm import tqdm
from rouge import Rouge # 모델의 성능을 평가하기 위한 라이브러리입니다.
from openai import OpenAI # openai==1.2.0
from dotenv import load_dotenv

# 스크립트 파일이 있는 디렉토리를 현재 작업 디렉토리로 설정
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# .env 파일에서 환경 변수 로드
load_dotenv()

# API 파라미터 글로벌 설정
params = {
    "model": "solar-1-mini-chat",
    "temperature": 0.2,
    "top_p": 0.3,
    "stream": False,
    "max_tokens": None  # 필요시 설정
}

log.info("""### 2) Solar Chat API Client 생성하기
- 앞으로 Solar Chat API를 사용하기 위해 Client를 생성합니다.
""")

# API 파라미터 로그 출력
log.info("API 파라미터 설정:")
for key, value in params.items():
    log.info(f"  {key}: {value}")

UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY") # .env 파일에서 API KEY를 읽어옵니다.

client = OpenAI(
    api_key=UPSTAGE_API_KEY,
    base_url="https://api.upstage.ai/v1/solar"
)

log.info("""### 3) Solar Chat API 사용해보기 (선택)
- 예시 코드를 통해 Solar Chat API를 사용해보세요.
""")

stream = client.chat.completions.create(
    model=params["model"],
    messages=[
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ],
    stream=True,
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")

# Use with stream=False
# log.info(stream.choices[0].message.content)

log.info("""### 4) 데이터 불러오기
- 실험에서 쓰일 데이터를 load합니다.
""")

# 데이터 경로를 지정해줍니다.
DATA_PATH = "../../input/data/"
RESULT_PATH = "./prediction/"

# train data의 구조와 내용을 확인합니다.
train_df = pd.read_csv(os.path.join(DATA_PATH,'train.csv'))
train_df.tail()

# validation data의 구조와 내용을 확인합니다.
val_df = pd.read_csv(os.path.join(DATA_PATH,'dev.csv'))
val_df.tail()

log.info("""## 1. Solar Chat API 요약 성능 확인하기
- Solar Chat API을 이용하여 train 및 validation dataset에 포함된 dialogue 샘플을 요약해 봅니다.
""")

# 모델 성능에 대한 평가 지표를 정의합니다. 본 대회에서는 ROUGE 점수를 통해 모델의 성능을 평가합니다.
rouge = Rouge()
def compute_metrics(pred, gold):
    results = rouge.get_scores(pred, gold, avg=True)
    result = {key: value["f"] for key, value in results.items()}
    return result

# Dialogue를 입력으로 받아, Solar Chat API에 보낼 Prompt를 생성하는 함수를 정의합니다.
def build_prompt(dialogue):
    system_prompt = "You are an expert in the field of dialogue summarization. Please summarize the following dialogue."

    user_prompt = f"Dialogue:\n{dialogue}\n\nSummary:\n"

    return [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": user_prompt
        }
    ]

# Solar Chat API를 활용해 Summarization을 수행하는 함수를 정의합니다.
def summarization(dialogue):
    summary = client.chat.completions.create(
        model=params["model"],
        messages=build_prompt(dialogue),
    )

    return summary.choices[0].message.content

log.info("""### (선택) parameter 변경하기
- Solar Chat API를 사용할 때, parameter를 변경하여, 다양한 결과를 얻을 수 있습니다.
- Parameter에 대한 자세한 설명은 [여기](https://developers.upstage.ai/docs/apis/chat#request-body)를 참고해주세요.
""")

def summarization(dialogue):
    # params에서 None이 아닌 값들만 사용
    api_params = {k: v for k, v in params.items() if v is not None}
    api_params["messages"] = build_prompt(dialogue)
    
    summary = client.chat.completions.create(**api_params)

    return summary.choices[0].message.content

log.info("""Train Dataset을 이용하여 요약이 잘 되는지 확인해 봅니다.""")

# Train data 중 처음 3개의 대화를 요약합니다.
def test_on_train_data(num_samples=3):
    for idx, row in train_df[:num_samples].iterrows():
        dialogue = row['dialogue']
        summary = summarization(dialogue)
        log.info(f"Dialogue:\n{dialogue}\n")
        log.info(f"Pred Summary: {summary}\n")
        log.info(f"Gold Summary: {row['summary']}\n")
        log.info("=="*50)

if __name__ == "__main__":
    test_on_train_data()

log.info("""Validation Dataset을 이용하여 요약을 진행하고, 성능을 평가해 봅니다.""")

# Validation data의 대화를 요약하고, 점수를 측정합니다.
def validate(num_samples=-1):
    val_samples = val_df[:num_samples] if num_samples > 0 else val_df

    # 기존 개별 점수 계산 방식 유지
    scores = []
    # 전체 배치 계산을 위한 리스트
    all_predictions = []
    all_labels = []
    
    for idx, row in tqdm(val_samples.iterrows(), total=len(val_samples)):
        dialogue = row['dialogue']
        summary = summarization(dialogue)
        results = compute_metrics(summary, row['summary'])
        avg_score = sum(results.values()) / len(results)

        scores.append(avg_score)
        all_predictions.append(summary)
        all_labels.append(row['summary'])

    # 기존 평균 점수 방식
    val_avg_score = sum(scores) / len(scores)
    log.info(f"Validation Average Score (개별 평균): {val_avg_score}")
    
    # baseline.py와 같은 방식의 상세한 ROUGE 분석
    log.info("="*50)
    log.info("상세한 ROUGE 메트릭 분석 (baseline.py 방식)")
    log.info("="*50)
    
    # 정확한 평가를 위해 미리 정의된 불필요한 생성토큰들을 제거합니다.
    # baseline.py의 remove_tokens와 동일하게 설정
    remove_tokens = ['<usr>', '<s>', '</s>', '<pad>']
    replaced_predictions = all_predictions.copy()
    replaced_labels = all_labels.copy()
    
    for token in remove_tokens:
        replaced_predictions = [sentence.replace(token, " ") for sentence in replaced_predictions]
        replaced_labels = [sentence.replace(token, " ") for sentence in replaced_labels]

    # 예측 결과와 정답 샘플 출력 (처음 3개)
    log.info('-'*150)
    for i in range(min(3, len(replaced_predictions))):
        log.info(f"PRED {i+1}: {replaced_predictions[i]}")
        log.info(f"GOLD {i+1}: {replaced_labels[i]}")
        log.info('-'*150)

    # 최종적인 ROUGE 점수를 계산합니다.
    rouge_evaluator = Rouge()
    results = rouge_evaluator.get_scores(replaced_predictions, replaced_labels, avg=True)
    
    # ROUGE 점수 결과를 로그에 출력
    log.info('-'*150)
    log.info("ROUGE Evaluation Results:")
    for metric_name, metric_values in results.items():
        log.info(f"{metric_name.upper()}: Precision={metric_values['p']:.4f}, Recall={metric_values['r']:.4f}, F1={metric_values['f']:.4f}")
    
    log.info('-'*150)
    
    # F1 점수들의 평균 계산
    f1_scores = [value["f"] for value in results.values()]
    batch_avg_score = sum(f1_scores) / len(f1_scores)
    log.info(f"Validation Average Score (전체 배치): {batch_avg_score}")
    
    return val_avg_score, batch_avg_score

if __name__ == "__main__":
    validate(100) # 100개의 validation sample에 대한 요약을 수행합니다.

    # 전체 validation data에 대한 요약을 수행하고 싶은 경우 아래와 같이 실행합니다.
    # validate()

log.info("""## 2. Solar Chat API로 요약하기
- Solar Chat API을 이용하여 test dataset에 포함된 dialogue를 요약하고 제출용 파일을 생성합니다.
""")

def inference(output_filename="output_solar.csv"):
    test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))

    summary = []
    start_time = time.time()
    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):
        dialogue = row['dialogue']
        summary.append(summarization(dialogue))

        # Rate limit 방지를 위해 1분 동안 최대 100개의 요청을 보내도록 합니다.
        if (idx + 1) % 100 == 0:
            end_time = time.time()
            elapsed_time = end_time - start_time

            if elapsed_time < 60:
                wait_time = 60 - elapsed_time + 5
                log.info(f"Elapsed time: {elapsed_time:.2f} sec")
                log.info(f"Waiting for {wait_time} sec")
                time.sleep(wait_time)

            start_time = time.time()

    output = pd.DataFrame(
        {
            "fname": test_df['fname'],
            "summary" : summary,
        }
    )

    if not os.path.exists(RESULT_PATH):
        os.makedirs(RESULT_PATH)
    output.to_csv(os.path.join(RESULT_PATH, output_filename), index=False)

    return output

if __name__ == "__main__":
    output = inference("output_solar.csv")

log.info(output)  # 각 대화문에 대한 요약문이 출력됨을 확인할 수 있습니다.

log.info("""## 3. Prompt Engineering
- Prompt engineering을 통해 요약 성능 향상을 시도합니다.
""")

# Few-shot prompt를 생성하기 위해, train data의 일부를 사용합니다.
few_shot_samples = train_df.sample(1)

sample_dialogue1 = few_shot_samples.iloc[0]['dialogue']
sample_summary1 = few_shot_samples.iloc[0]['summary']

log.info(f"Sample Dialogue1:\n{sample_dialogue1}\n")
log.info(f"Sample Summary1: {sample_summary1}\n")

# Prompt를 생성하는 함수를 수정합니다.
def build_prompt(dialogue):
    system_prompt = "You are a expert in the field of dialogue summarization, summarize the given dialogue in a concise manner. Follow the user's instruction carefully and provide a summary that is relevant to the dialogue."

    user_prompt = (
        "Following the instructions below, summarize the given document.\n"
        "Instructions:\n"
        "1. Read the provided sample dialogue and corresponding summary.\n"
        "2. Read the dialogue carefully.\n"
        "3. Following the sample's style of summary, provide a concise summary of the given dialogue.\n\n"
        "Sample Dialogue:\n"
        f"{sample_dialogue1}\n\n"
        "Sample Summary:\n"
        f"{sample_summary1}\n\n"
        "Dialogue:\n"
        f"{dialogue}\n\n"
        "Summary:\n"
    )

    return [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": user_prompt
        }
    ]

# 변경된 prompt를 사용하여, train data 중 처음 3개의 대화를 요약하고, 결과를 확인합니다.
if __name__ == "__main__":
    test_on_train_data()

# 변경된 prompt를 사용하여, validation data의 대화를 요약하고, 점수를 측정합니다.
if __name__ == "__main__":
    validate(100)
    
# 첫 번째 퓨샷 방식으로 test dataset에 대한 추론을 수행합니다.
if __name__ == "__main__":
    output = inference("output_solar_fewshot1.csv")

log.info("""다른 방식으로 Few-shot sample을 제공하여 Prompt를 구성해 봅니다.""")

# Few-shot sample을 다른 방식으로 사용하여 prompt를 생성합니다.
def build_prompt(dialogue):
    system_prompt = "You are a expert in the field of dialogue summarization, summarize the given dialogue in a concise manner. Follow the user's instruction carefully and provide a summary that is relevant to the dialogue."

    few_shot_user_prompt_1 = (
        "Following the instructions below, summarize the given document.\n"
        "Instructions:\n"
        "1. Read the provided sample dialogue and corresponding summary.\n"
        "2. Read the dialogue carefully.\n"
        "3. Following the sample's style of summary, provide a concise summary of the given dialogue. Be sure that the summary is simple but captures the essence of the dialogue.\n\n"
        "Dialogue:\n"
        f"{sample_dialogue1}\n\n"
        "Summary:\n"
    )
    few_shot_assistant_prompt_1 = sample_summary1

    user_prompt = (
        "Dialogue:\n"
        f"{dialogue}\n\n"
        "Summary:\n"
    )

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": few_shot_user_prompt_1},
        {"role": "assistant", "content": few_shot_assistant_prompt_1},
        {"role": "user", "content": user_prompt},
    ]

# 변경된 prompt를 사용하여, train data 중 처음 3개의 대화를 요약하고, 결과를 확인합니다.
if __name__ == "__main__":
    test_on_train_data()

# 변경된 prompt를 사용하여, validation data의 대화를 요약하고, 점수를 측정합니다.
if __name__ == "__main__":
    validate(100)

log.info("""### (선택) 변경된 Prompt로 test dataset에 대한 요약을 진행합니다.
- 변경된 prompt를 통해 점수가 개선되었다면, test dataset에 대한 요약을 진행하고 제출합니다.
""")

# 변경된 prompt를 사용하여, test data의 대화를 요약하고, 결과를 확인합니다.
if __name__ == "__main__":
    output = inference("output_solar_fewshot2.csv")

log.info(output)