# -*- coding: utf-8 -*-

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
import os; os.chdir(os.path.dirname(os.path.abspath(__file__)))
import sys; sys.path.append('../utils')
import log_util as log

log.info("""solar_api.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZDwSFj5xh18hfTOmIocChNSwY_Scq1gj

# **ğŸ’ğŸ»ğŸ—¨ï¸ğŸ’ğŸ»â€â™‚ï¸ëŒ€í™” ìš”ì•½ SOLAR API code**
> **Dialogue Summarization** ê²½ì§„ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰    
> ë³¸ ìë£Œì—ì„œëŠ” Solar Chat APIë¥¼ ì´ìš©í•˜ì—¬ ëŒ€í™” ìš”ì•½ ëŒ€íšŒë¥¼ í’€ì–´ë´…ë‹ˆë‹¤.

## âš™ï¸ ë°ì´í„° ë° í™˜ê²½ì„¤ì •

### 1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œ í›„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
""")

# !pip install openai

import pandas as pd
import os
import time
from tqdm import tqdm
from rouge import Rouge # ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
from openai import OpenAI # openai==1.2.0
from dotenv import load_dotenv

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API íŒŒë¼ë¯¸í„° ê¸€ë¡œë²Œ ì„¤ì •
params = {
    "model": "solar-1-mini-chat",
    "temperature": 0.2,
    "top_p": 0.3,
    "stream": False,
    "max_tokens": None  # í•„ìš”ì‹œ ì„¤ì •
}

log.info("""### 2) Solar Chat API Client ìƒì„±í•˜ê¸°
- ì•ìœ¼ë¡œ Solar Chat APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Clientë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
""")

# API íŒŒë¼ë¯¸í„° ë¡œê·¸ ì¶œë ¥
log.info("API íŒŒë¼ë¯¸í„° ì„¤ì •:")
for key, value in params.items():
    log.info(f"  {key}: {value}")

UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY") # .env íŒŒì¼ì—ì„œ API KEYë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.

client = OpenAI(
    api_key=UPSTAGE_API_KEY,
    base_url="https://api.upstage.ai/v1/solar"
)

log.info("""### 3) Solar Chat API ì‚¬ìš©í•´ë³´ê¸° (ì„ íƒ)
- ì˜ˆì‹œ ì½”ë“œë¥¼ í†µí•´ Solar Chat APIë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.
""")

stream = client.chat.completions.create(
    model=params["model"],
    messages=[
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ],
    stream=True,
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")

# Use with stream=False
# log.info(stream.choices[0].message.content)

log.info("""### 4) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
- ì‹¤í—˜ì—ì„œ ì“°ì¼ ë°ì´í„°ë¥¼ loadí•©ë‹ˆë‹¤.
""")

# ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì •í•´ì¤ë‹ˆë‹¤.
DATA_PATH = "../../input/data/"
RESULT_PATH = "./prediction/"

# train dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.
train_df = pd.read_csv(os.path.join(DATA_PATH,'train.csv'))
train_df.tail()

# validation dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.
val_df = pd.read_csv(os.path.join(DATA_PATH,'dev.csv'))
val_df.tail()

log.info("""## 1. Solar Chat API ìš”ì•½ ì„±ëŠ¥ í™•ì¸í•˜ê¸°
- Solar Chat APIì„ ì´ìš©í•˜ì—¬ train ë° validation datasetì— í¬í•¨ëœ dialogue ìƒ˜í”Œì„ ìš”ì•½í•´ ë´…ë‹ˆë‹¤.
""")

# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
rouge = Rouge()
def compute_metrics(pred, gold):
    results = rouge.get_scores(pred, gold, avg=True)
    result = {key: value["f"] for key, value in results.items()}
    return result

# Dialogueë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, Solar Chat APIì— ë³´ë‚¼ Promptë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
def build_prompt(dialogue):
    system_prompt = "You are an expert in the field of dialogue summarization. Please summarize the following dialogue."

    user_prompt = f"Dialogue:\n{dialogue}\n\nSummary:\n"

    return [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": user_prompt
        }
    ]

# Solar Chat APIë¥¼ í™œìš©í•´ Summarizationì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
def summarization(dialogue):
    summary = client.chat.completions.create(
        model=params["model"],
        messages=build_prompt(dialogue),
    )

    return summary.choices[0].message.content

log.info("""### (ì„ íƒ) parameter ë³€ê²½í•˜ê¸°
- Solar Chat APIë¥¼ ì‚¬ìš©í•  ë•Œ, parameterë¥¼ ë³€ê²½í•˜ì—¬, ë‹¤ì–‘í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Parameterì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [ì—¬ê¸°](https://developers.upstage.ai/docs/apis/chat#request-body)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.
""")

def summarization(dialogue):
    # paramsì—ì„œ Noneì´ ì•„ë‹Œ ê°’ë“¤ë§Œ ì‚¬ìš©
    api_params = {k: v for k, v in params.items() if v is not None}
    api_params["messages"] = build_prompt(dialogue)
    
    summary = client.chat.completions.create(**api_params)

    return summary.choices[0].message.content

log.info("""Train Datasetì„ ì´ìš©í•˜ì—¬ ìš”ì•½ì´ ì˜ ë˜ëŠ”ì§€ í™•ì¸í•´ ë´…ë‹ˆë‹¤.""")

# Train data ì¤‘ ì²˜ìŒ 3ê°œì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
def test_on_train_data(num_samples=3):
    for idx, row in train_df[:num_samples].iterrows():
        dialogue = row['dialogue']
        summary = summarization(dialogue)
        log.info(f"Dialogue:\n{dialogue}\n")
        log.info(f"Pred Summary: {summary}\n")
        log.info(f"Gold Summary: {row['summary']}\n")
        log.info("=="*50)

if __name__ == "__main__":
    test_on_train_data()

log.info("""Validation Datasetì„ ì´ìš©í•˜ì—¬ ìš”ì•½ì„ ì§„í–‰í•˜ê³ , ì„±ëŠ¥ì„ í‰ê°€í•´ ë´…ë‹ˆë‹¤.""")

# Validation dataì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ì ìˆ˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
def validate(num_samples=-1):
    val_samples = val_df[:num_samples] if num_samples > 0 else val_df

    # ê¸°ì¡´ ê°œë³„ ì ìˆ˜ ê³„ì‚° ë°©ì‹ ìœ ì§€
    scores = []
    # ì „ì²´ ë°°ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    all_predictions = []
    all_labels = []
    
    for idx, row in tqdm(val_samples.iterrows(), total=len(val_samples)):
        dialogue = row['dialogue']
        summary = summarization(dialogue)
        results = compute_metrics(summary, row['summary'])
        avg_score = sum(results.values()) / len(results)

        scores.append(avg_score)
        all_predictions.append(summary)
        all_labels.append(row['summary'])

    # ê¸°ì¡´ í‰ê·  ì ìˆ˜ ë°©ì‹
    val_avg_score = sum(scores) / len(scores)
    log.info(f"Validation Average Score (ê°œë³„ í‰ê· ): {val_avg_score}")
    
    # baseline.pyì™€ ê°™ì€ ë°©ì‹ì˜ ìƒì„¸í•œ ROUGE ë¶„ì„
    log.info("="*50)
    log.info("ìƒì„¸í•œ ROUGE ë©”íŠ¸ë¦­ ë¶„ì„ (baseline.py ë°©ì‹)")
    log.info("="*50)
    
    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
    # baseline.pyì˜ remove_tokensì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
    remove_tokens = ['<usr>', '<s>', '</s>', '<pad>']
    replaced_predictions = all_predictions.copy()
    replaced_labels = all_labels.copy()
    
    for token in remove_tokens:
        replaced_predictions = [sentence.replace(token, " ") for sentence in replaced_predictions]
        replaced_labels = [sentence.replace(token, " ") for sentence in replaced_labels]

    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 3ê°œ)
    log.info('-'*150)
    for i in range(min(3, len(replaced_predictions))):
        log.info(f"PRED {i+1}: {replaced_predictions[i]}")
        log.info(f"GOLD {i+1}: {replaced_labels[i]}")
        log.info('-'*150)

    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    rouge_evaluator = Rouge()
    results = rouge_evaluator.get_scores(replaced_predictions, replaced_labels, avg=True)
    
    # ROUGE ì ìˆ˜ ê²°ê³¼ë¥¼ ë¡œê·¸ì— ì¶œë ¥
    log.info('-'*150)
    log.info("ROUGE Evaluation Results:")
    for metric_name, metric_values in results.items():
        log.info(f"{metric_name.upper()}: Precision={metric_values['p']:.4f}, Recall={metric_values['r']:.4f}, F1={metric_values['f']:.4f}")
    
    log.info('-'*150)
    
    # F1 ì ìˆ˜ë“¤ì˜ í‰ê·  ê³„ì‚°
    f1_scores = [value["f"] for value in results.values()]
    batch_avg_score = sum(f1_scores) / len(f1_scores)
    log.info(f"Validation Average Score (ì „ì²´ ë°°ì¹˜): {batch_avg_score}")
    
    return val_avg_score, batch_avg_score

if __name__ == "__main__":
    validate(100) # 100ê°œì˜ validation sampleì— ëŒ€í•œ ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    # ì „ì²´ validation dataì— ëŒ€í•œ ìš”ì•½ì„ ìˆ˜í–‰í•˜ê³  ì‹¶ì€ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.
    # validate()

log.info("""## 2. Solar Chat APIë¡œ ìš”ì•½í•˜ê¸°
- Solar Chat APIì„ ì´ìš©í•˜ì—¬ test datasetì— í¬í•¨ëœ dialogueë¥¼ ìš”ì•½í•˜ê³  ì œì¶œìš© íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
""")

def inference(output_filename="output_solar.csv"):
    test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))

    summary = []
    start_time = time.time()
    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):
        dialogue = row['dialogue']
        summary.append(summarization(dialogue))

        # Rate limit ë°©ì§€ë¥¼ ìœ„í•´ 1ë¶„ ë™ì•ˆ ìµœëŒ€ 100ê°œì˜ ìš”ì²­ì„ ë³´ë‚´ë„ë¡ í•©ë‹ˆë‹¤.
        if (idx + 1) % 100 == 0:
            end_time = time.time()
            elapsed_time = end_time - start_time

            if elapsed_time < 60:
                wait_time = 60 - elapsed_time + 5
                log.info(f"Elapsed time: {elapsed_time:.2f} sec")
                log.info(f"Waiting for {wait_time} sec")
                time.sleep(wait_time)

            start_time = time.time()

    output = pd.DataFrame(
        {
            "fname": test_df['fname'],
            "summary" : summary,
        }
    )

    if not os.path.exists(RESULT_PATH):
        os.makedirs(RESULT_PATH)
    output.to_csv(os.path.join(RESULT_PATH, output_filename), index=False)

    return output

if __name__ == "__main__":
    output = inference("output_solar.csv")

log.info(output)  # ê° ëŒ€í™”ë¬¸ì— ëŒ€í•œ ìš”ì•½ë¬¸ì´ ì¶œë ¥ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

log.info("""## 3. Prompt Engineering
- Prompt engineeringì„ í†µí•´ ìš”ì•½ ì„±ëŠ¥ í–¥ìƒì„ ì‹œë„í•©ë‹ˆë‹¤.
""")

# Few-shot promptë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´, train dataì˜ ì¼ë¶€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
few_shot_samples = train_df.sample(1)

sample_dialogue1 = few_shot_samples.iloc[0]['dialogue']
sample_summary1 = few_shot_samples.iloc[0]['summary']

log.info(f"Sample Dialogue1:\n{sample_dialogue1}\n")
log.info(f"Sample Summary1: {sample_summary1}\n")

# Promptë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
def build_prompt(dialogue):
    system_prompt = "You are a expert in the field of dialogue summarization, summarize the given dialogue in a concise manner. Follow the user's instruction carefully and provide a summary that is relevant to the dialogue."

    user_prompt = (
        "Following the instructions below, summarize the given document.\n"
        "Instructions:\n"
        "1. Read the provided sample dialogue and corresponding summary.\n"
        "2. Read the dialogue carefully.\n"
        "3. Following the sample's style of summary, provide a concise summary of the given dialogue.\n\n"
        "Sample Dialogue:\n"
        f"{sample_dialogue1}\n\n"
        "Sample Summary:\n"
        f"{sample_summary1}\n\n"
        "Dialogue:\n"
        f"{dialogue}\n\n"
        "Summary:\n"
    )

    return [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": user_prompt
        }
    ]

# ë³€ê²½ëœ promptë¥¼ ì‚¬ìš©í•˜ì—¬, train data ì¤‘ ì²˜ìŒ 3ê°œì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    test_on_train_data()

# ë³€ê²½ëœ promptë¥¼ ì‚¬ìš©í•˜ì—¬, validation dataì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ì ìˆ˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    validate(100)
    
# ì²« ë²ˆì§¸ í“¨ìƒ· ë°©ì‹ìœ¼ë¡œ test datasetì— ëŒ€í•œ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    output = inference("output_solar_fewshot1.csv")

log.info("""ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ Few-shot sampleì„ ì œê³µí•˜ì—¬ Promptë¥¼ êµ¬ì„±í•´ ë´…ë‹ˆë‹¤.""")

# Few-shot sampleì„ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ promptë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
def build_prompt(dialogue):
    system_prompt = "You are a expert in the field of dialogue summarization, summarize the given dialogue in a concise manner. Follow the user's instruction carefully and provide a summary that is relevant to the dialogue."

    few_shot_user_prompt_1 = (
        "Following the instructions below, summarize the given document.\n"
        "Instructions:\n"
        "1. Read the provided sample dialogue and corresponding summary.\n"
        "2. Read the dialogue carefully.\n"
        "3. Following the sample's style of summary, provide a concise summary of the given dialogue. Be sure that the summary is simple but captures the essence of the dialogue.\n\n"
        "Dialogue:\n"
        f"{sample_dialogue1}\n\n"
        "Summary:\n"
    )
    few_shot_assistant_prompt_1 = sample_summary1

    user_prompt = (
        "Dialogue:\n"
        f"{dialogue}\n\n"
        "Summary:\n"
    )

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": few_shot_user_prompt_1},
        {"role": "assistant", "content": few_shot_assistant_prompt_1},
        {"role": "user", "content": user_prompt},
    ]

# ë³€ê²½ëœ promptë¥¼ ì‚¬ìš©í•˜ì—¬, train data ì¤‘ ì²˜ìŒ 3ê°œì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    test_on_train_data()

# ë³€ê²½ëœ promptë¥¼ ì‚¬ìš©í•˜ì—¬, validation dataì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ì ìˆ˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    validate(100)

log.info("""### (ì„ íƒ) ë³€ê²½ëœ Promptë¡œ test datasetì— ëŒ€í•œ ìš”ì•½ì„ ì§„í–‰í•©ë‹ˆë‹¤.
- ë³€ê²½ëœ promptë¥¼ í†µí•´ ì ìˆ˜ê°€ ê°œì„ ë˜ì—ˆë‹¤ë©´, test datasetì— ëŒ€í•œ ìš”ì•½ì„ ì§„í–‰í•˜ê³  ì œì¶œí•©ë‹ˆë‹¤.
""")

# ë³€ê²½ëœ promptë¥¼ ì‚¬ìš©í•˜ì—¬, test dataì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    output = inference("output_solar_fewshot2.csv")

log.info(output)