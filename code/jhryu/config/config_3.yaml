general:
  data_path: ../../input/data/
  model_name: digit82/kobart-summarization
  output_dir: ./
inference:
  batch_size: 32
  ckt_path: model ckt path
  early_stopping: true
  generate_max_length: 109
  no_repeat_ngram_size: 4
  num_beams: 10
  remove_tokens:
  - <usr>
  - <s>
  - </s>
  - <pad>
  result_path: ./prediction/
tokenizer:
  bos_token: <s>
  decoder_max_len: 120
  encoder_max_len: 456
  eos_token: </s>
  special_tokens:
  - '#Person1#'
  - '#Person2#'
  - '#Person3#'
  - '#PhoneNumber#'
  - '#Address#'
  - '#PassportNumber#'
training:
  do_eval: true
  do_train: true
  early_stopping_patience: 9
  early_stopping_threshold: 0.003762661504716025
  evaluation_strategy: epoch
  fp16: true
  generation_max_length: 60
  gradient_accumulation_steps: 1
  learning_rate: 1.1008198601730776e-05
  load_best_model_at_end: true
  logging_dir: ./logs
  logging_strategy: epoch
  lr_scheduler_type: linear
  num_train_epochs: 7
  optim: adafactor
  overwrite_output_dir: true
  per_device_eval_batch_size: 32
  per_device_train_batch_size: 8
  predict_with_generate: true
  report_to: wandb
  save_strategy: epoch
  save_total_limit: 5
  seed: 204518
  use_early_stopping: false
  warmup_ratio: 0.13554203512946492
  weight_decay: 0.09702103051300771
wandb:
  entity: moonstalker9010-none
  name: jhryu_baseline_exp
  project: NLP
